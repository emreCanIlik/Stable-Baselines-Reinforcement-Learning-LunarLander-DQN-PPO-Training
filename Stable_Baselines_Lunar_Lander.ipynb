{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training DQN and PPO Agents on the LunarLander Environment: A Comparative Analysis"
      ],
      "metadata": {
        "id": "WYcKJrdf_Nlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install swig\n",
        "!pip install box2d-py\n",
        "!apt-get update\n",
        "!apt-get install cmake libopenmpi-dev zlib1g-dev\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install pyvirtualdisplay"
      ],
      "metadata": {
        "id": "01L7yNsdF88n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3[extra]"
      ],
      "metadata": {
        "id": "_x2JdsKBQzNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"RL_Agent\")"
      ],
      "metadata": {
        "id": "UY7AcMsW_NGB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile RL_Agent/env_creator.py\n",
        "\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "def create_env(env_name: str, n_envs: int):\n",
        "    \"\"\"Creates environment for RL experiments.\n",
        "    Takes in environment name and number of environments, and\n",
        "    returns environment and evaluation environment.\"\"\"\n",
        "\n",
        "    env = make_vec_env(env_name, n_envs)\n",
        "    eval_env = make_vec_env(env_name, n_envs)\n",
        "\n",
        "    return env, eval_env\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Fm--6eY_t8D",
        "outputId": "2e9bc65a-d1d9-49fc-fccf-91a3a9f6b78b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing RL_Agent/env_creator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A942mxG-Gp0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = \"LunarLander-v2\"\n",
        "n_envs = 4"
      ],
      "metadata": {
        "id": "zvhE-KfYBzTs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from RL_Agent import env_creator\n",
        "\n",
        "env, eval_env = env_creator.create_env(env_name=env_name, n_envs=n_envs)"
      ],
      "metadata": {
        "id": "dZItw8pbB1tP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env, eval_env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_efPBO_THJjJ",
        "outputId": "dd52886b-63df-48fa-bde7-60470793f23e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv at 0x7e42e0a1e350>,\n",
              " <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv at 0x7e42e0a1e9b0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(env), type(eval_env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx4aUJ-8HKwS",
        "outputId": "4f47f7c6-46e7-4b2b-d18a-e81e3c487d52"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv,\n",
              " stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ],
      "metadata": {
        "id": "U2DnWHnwHOeO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_callback = EvalCallback(eval_env, best_model_save_path='Eval_logs_best_DQN',\n",
        "                                 log_path='Eval_logs_DQN', eval_freq=10,\n",
        "                                 verbose=1)"
      ],
      "metadata": {
        "id": "5IogrSFvHsaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile RL_Agent/evaluator.py\n",
        "\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "\n",
        "def create_eval_callback(eval_env, best_model_save_path: str, log_path: str,\n",
        "                         eval_freq: int, verbose: int):\n",
        "\n",
        "    \"\"\"Creates an evaluation callback for RL training.\"\"\"\n",
        "    eval_callback = EvalCallback(\n",
        "        eval_env=eval_env,\n",
        "        best_model_save_path=best_model_save_path,\n",
        "        log_path=log_path,\n",
        "        eval_freq=eval_freq,\n",
        "        verbose=verbose\n",
        "    )\n",
        "    return eval_callback\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWHLlj0FHs7l",
        "outputId": "0b68527c-6648-4e70-e3f4-0155ddf52c6a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing RL_Agent/evaluator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_save_path = \"best_model\"\n",
        "log_path = \"logs\"\n",
        "eval_freq = 100\n",
        "verbose = 1"
      ],
      "metadata": {
        "id": "W15H1Yu-I_l1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from RL_Agent import evaluator\n",
        "\n",
        "eval_callback = evaluator.create_eval_callback(eval_env = eval_env,\n",
        "                                               best_model_save_path=best_model_save_path,\n",
        "                                               log_path=log_path,\n",
        "                                               eval_freq=eval_freq,\n",
        "                                               verbose=verbose)"
      ],
      "metadata": {
        "id": "sCpHNF0YJU3R"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_callback"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsXPzHeHJuuZ",
        "outputId": "2704f025-7b5d-4b2f-9450-2c75323ed36b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.common.callbacks.EvalCallback at 0x7e42e0a1cd30>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile RL_Agent/agent_creator.py\n",
        "\n",
        "import torch\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "def create_agents(policy: str, env, learning_rate: float,\n",
        "                     tensorboard_log: str, verbose: int, device: torch.device):\n",
        "\n",
        "    \"\"\" Creates DQN and PPO agents \"\"\"\n",
        "\n",
        "    agent_dqn = DQN(policy, env, learning_rate=learning_rate,\n",
        "                tensorboard_log=tensorboard_log, verbose=verbose, device=device)\n",
        "\n",
        "    agent_ppo = PPO(policy, env, learning_rate=learning_rate,\n",
        "                tensorboard_log=tensorboard_log, verbose=verbose, device=device)\n",
        "\n",
        "\n",
        "    return agent_dqn, agent_ppo\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMtoNOltJvvX",
        "outputId": "965110ba-3ca6-4b47-a077-a675862f664f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing RL_Agent/agent_creator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "policy = \"MlpPolicy\"\n",
        "env = env\n",
        "learning_rate = 0.0001\n",
        "tensorboard_log = \"logs_tensorboard\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "y5WOlWA8Mzhp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from RL_Agent import agent_creator\n",
        "\n",
        "dqn_agent, ppo_agent = agent_creator.create_agents(policy=policy,\n",
        "                            env=env,\n",
        "                            learning_rate=learning_rate,\n",
        "                            tensorboard_log=tensorboard_log,\n",
        "                            device=device,\n",
        "                            verbose=verbose)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLnjsg1GKYrs",
        "outputId": "42a5e40e-9715-41e5-c7aa-a038183b1086"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dqn_agent, ppo_agent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGwhQGOaOn0N",
        "outputId": "5134dbf1-6333-412f-82a2-815ea28ce43f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<stable_baselines3.dqn.dqn.DQN at 0x7e42e0a1d0f0>,\n",
              " <stable_baselines3.ppo.ppo.PPO at 0x7e418fd1ddb0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile RL_Agent/engine.py\n",
        "\n",
        "def train_agent(agent, total_timesteps: int, callback, tb_log_name: str):\n",
        "\n",
        "    \"\"\"Train the agent with specified parameters.\"\"\"\n",
        "\n",
        "    agent.learn(total_timesteps=total_timesteps, callback=callback, tb_log_name=tb_log_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t-rfw_5zZWW",
        "outputId": "134bbd65-e368-4ec6-9e84-1a6c2e6fe5dd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing RL_Agent/engine.py\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tb_log_name = \"runs\"\n",
        "callback = eval_callback\n",
        "total_timesteps = 100_000"
      ],
      "metadata": {
        "id": "2bWJ-BHhz1TN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from RL_Agent import engine\n",
        "\n",
        "# Train the DQN agent\n",
        "engine.train_agent(agent=dqn_agent,\n",
        "                   total_timesteps=total_timesteps,\n",
        "                   callback=callback,\n",
        "                   tb_log_name=tb_log_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZlR_2Y51Q_F",
        "outputId": "ef6644a7-322f-41a5-a2a4-3863fe36c771"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to logs_tensorboard/runs_3\n",
            "Eval num_timesteps=400, episode_reward=-21.75 +/- 158.80\n",
            "Episode length: 786.20 +/- 262.81\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 786      |\n",
            "|    mean_reward      | -21.8    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.962    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 400      |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.51     |\n",
            "|    n_updates        | 6355     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 97.5     |\n",
            "|    ep_rew_mean      | -133     |\n",
            "|    exploration_rate | 0.956    |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 61       |\n",
            "|    time_elapsed     | 7        |\n",
            "|    total_timesteps  | 468      |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.38     |\n",
            "|    n_updates        | 6360     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=800, episode_reward=-159.68 +/- 91.77\n",
            "Episode length: 553.00 +/- 237.34\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 553      |\n",
            "|    mean_reward      | -160     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.924    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 800      |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.645    |\n",
            "|    n_updates        | 6380     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.6     |\n",
            "|    ep_rew_mean      | -126     |\n",
            "|    exploration_rate | 0.913    |\n",
            "| time/               |          |\n",
            "|    episodes         | 8        |\n",
            "|    fps              | 62       |\n",
            "|    time_elapsed     | 14       |\n",
            "|    total_timesteps  | 912      |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.29     |\n",
            "|    n_updates        | 6387     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1200, episode_reward=97.45 +/- 121.53\n",
            "Episode length: 511.20 +/- 253.59\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 511      |\n",
            "|    mean_reward      | 97.5     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.886    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 1200     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.35     |\n",
            "|    n_updates        | 6405     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.1     |\n",
            "|    ep_rew_mean      | -136     |\n",
            "|    exploration_rate | 0.864    |\n",
            "| time/               |          |\n",
            "|    episodes         | 12       |\n",
            "|    fps              | 66       |\n",
            "|    time_elapsed     | 21       |\n",
            "|    total_timesteps  | 1428     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.74     |\n",
            "|    n_updates        | 6420     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1600, episode_reward=14.72 +/- 169.90\n",
            "Episode length: 482.20 +/- 214.58\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 482      |\n",
            "|    mean_reward      | 14.7     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.848    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 1600     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.977    |\n",
            "|    n_updates        | 6430     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.833    |\n",
            "| time/               |          |\n",
            "|    episodes         | 16       |\n",
            "|    fps              | 69       |\n",
            "|    time_elapsed     | 25       |\n",
            "|    total_timesteps  | 1756     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.33     |\n",
            "|    n_updates        | 6440     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2000, episode_reward=21.48 +/- 152.74\n",
            "Episode length: 578.40 +/- 216.67\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 578      |\n",
            "|    mean_reward      | 21.5     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.81     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 2000     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.05     |\n",
            "|    n_updates        | 6455     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 97.6     |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.804    |\n",
            "| time/               |          |\n",
            "|    episodes         | 20       |\n",
            "|    fps              | 64       |\n",
            "|    time_elapsed     | 31       |\n",
            "|    total_timesteps  | 2068     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.69     |\n",
            "|    n_updates        | 6460     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2400, episode_reward=-118.32 +/- 82.00\n",
            "Episode length: 497.40 +/- 110.44\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 497      |\n",
            "|    mean_reward      | -118     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.772    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 2400     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.745    |\n",
            "|    n_updates        | 6480     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99.1     |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.767    |\n",
            "| time/               |          |\n",
            "|    episodes         | 24       |\n",
            "|    fps              | 69       |\n",
            "|    time_elapsed     | 35       |\n",
            "|    total_timesteps  | 2456     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.42     |\n",
            "|    n_updates        | 6484     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2800, episode_reward=12.43 +/- 154.68\n",
            "Episode length: 703.20 +/- 227.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 703      |\n",
            "|    mean_reward      | 12.4     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.734    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 2800     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.962    |\n",
            "|    n_updates        | 6505     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 100      |\n",
            "|    ep_rew_mean      | -118     |\n",
            "|    exploration_rate | 0.73     |\n",
            "| time/               |          |\n",
            "|    episodes         | 28       |\n",
            "|    fps              | 70       |\n",
            "|    time_elapsed     | 40       |\n",
            "|    total_timesteps  | 2844     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.78     |\n",
            "|    n_updates        | 6508     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3200, episode_reward=-107.13 +/- 73.58\n",
            "Episode length: 564.40 +/- 238.11\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 564      |\n",
            "|    mean_reward      | -107     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.696    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 3200     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.24     |\n",
            "|    n_updates        | 6530     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 104      |\n",
            "|    ep_rew_mean      | -118     |\n",
            "|    exploration_rate | 0.664    |\n",
            "| time/               |          |\n",
            "|    episodes         | 32       |\n",
            "|    fps              | 80       |\n",
            "|    time_elapsed     | 43       |\n",
            "|    total_timesteps  | 3536     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.56     |\n",
            "|    n_updates        | 6551     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3600, episode_reward=49.50 +/- 87.62\n",
            "Episode length: 474.00 +/- 278.86\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 474      |\n",
            "|    mean_reward      | 49.5     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.658    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 3600     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.37     |\n",
            "|    n_updates        | 6555     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 105      |\n",
            "|    ep_rew_mean      | -118     |\n",
            "|    exploration_rate | 0.626    |\n",
            "| time/               |          |\n",
            "|    episodes         | 36       |\n",
            "|    fps              | 83       |\n",
            "|    time_elapsed     | 47       |\n",
            "|    total_timesteps  | 3936     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.16     |\n",
            "|    n_updates        | 6576     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=4000, episode_reward=-34.64 +/- 204.84\n",
            "Episode length: 511.80 +/- 279.05\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 512      |\n",
            "|    mean_reward      | -34.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.62     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 4000     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.735    |\n",
            "|    n_updates        | 6580     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=4400, episode_reward=53.71 +/- 62.12\n",
            "Episode length: 537.00 +/- 284.86\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 537      |\n",
            "|    mean_reward      | 53.7     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.582    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 4400     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.49     |\n",
            "|    n_updates        | 6605     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 107      |\n",
            "|    ep_rew_mean      | -111     |\n",
            "|    exploration_rate | 0.561    |\n",
            "| time/               |          |\n",
            "|    episodes         | 40       |\n",
            "|    fps              | 82       |\n",
            "|    time_elapsed     | 55       |\n",
            "|    total_timesteps  | 4620     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.875    |\n",
            "|    n_updates        | 6619     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=4800, episode_reward=93.76 +/- 109.74\n",
            "Episode length: 373.20 +/- 134.73\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 373      |\n",
            "|    mean_reward      | 93.8     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.544    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 4800     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.06     |\n",
            "|    n_updates        | 6630     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 112      |\n",
            "|    ep_rew_mean      | -106     |\n",
            "|    exploration_rate | 0.511    |\n",
            "| time/               |          |\n",
            "|    episodes         | 44       |\n",
            "|    fps              | 88       |\n",
            "|    time_elapsed     | 58       |\n",
            "|    total_timesteps  | 5148     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.819    |\n",
            "|    n_updates        | 6652     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=5200, episode_reward=95.21 +/- 135.74\n",
            "Episode length: 607.60 +/- 263.56\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 608      |\n",
            "|    mean_reward      | 95.2     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.506    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 5200     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.2      |\n",
            "|    n_updates        | 6655     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=5600, episode_reward=43.75 +/- 156.68\n",
            "Episode length: 614.00 +/- 330.88\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 614      |\n",
            "|    mean_reward      | 43.7     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.468    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 5600     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.726    |\n",
            "|    n_updates        | 6680     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=6000, episode_reward=-25.52 +/- 105.91\n",
            "Episode length: 443.60 +/- 94.29\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 444      |\n",
            "|    mean_reward      | -25.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.43     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 6000     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.2      |\n",
            "|    n_updates        | 6705     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 118      |\n",
            "|    ep_rew_mean      | -101     |\n",
            "|    exploration_rate | 0.419    |\n",
            "| time/               |          |\n",
            "|    episodes         | 48       |\n",
            "|    fps              | 83       |\n",
            "|    time_elapsed     | 73       |\n",
            "|    total_timesteps  | 6116     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.934    |\n",
            "|    n_updates        | 6713     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=6400, episode_reward=-2.93 +/- 127.82\n",
            "Episode length: 759.40 +/- 303.90\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 759      |\n",
            "|    mean_reward      | -2.93    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.392    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 6400     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.786    |\n",
            "|    n_updates        | 6730     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=6800, episode_reward=56.14 +/- 81.27\n",
            "Episode length: 613.60 +/- 328.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 614      |\n",
            "|    mean_reward      | 56.1     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.354    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 6800     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.983    |\n",
            "|    n_updates        | 6755     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=7200, episode_reward=87.39 +/- 85.15\n",
            "Episode length: 411.80 +/- 105.13\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 412      |\n",
            "|    mean_reward      | 87.4     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.316    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 7200     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.74     |\n",
            "|    n_updates        | 6780     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 129      |\n",
            "|    ep_rew_mean      | -106     |\n",
            "|    exploration_rate | 0.29     |\n",
            "| time/               |          |\n",
            "|    episodes         | 52       |\n",
            "|    fps              | 84       |\n",
            "|    time_elapsed     | 88       |\n",
            "|    total_timesteps  | 7476     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.603    |\n",
            "|    n_updates        | 6798     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=7600, episode_reward=-52.22 +/- 52.14\n",
            "Episode length: 329.00 +/- 92.36\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 329      |\n",
            "|    mean_reward      | -52.2    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.278    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 7600     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.609    |\n",
            "|    n_updates        | 6805     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=8000, episode_reward=39.09 +/- 154.99\n",
            "Episode length: 592.40 +/- 336.84\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 592      |\n",
            "|    mean_reward      | 39.1     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.24     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 8000     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.28     |\n",
            "|    n_updates        | 6830     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=8400, episode_reward=-8.07 +/- 129.92\n",
            "Episode length: 412.00 +/- 92.61\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 412      |\n",
            "|    mean_reward      | -8.07    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.202    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 8400     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.724    |\n",
            "|    n_updates        | 6855     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=8800, episode_reward=-69.54 +/- 192.25\n",
            "Episode length: 501.40 +/- 256.49\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 501      |\n",
            "|    mean_reward      | -69.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.164    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 8800     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.06     |\n",
            "|    n_updates        | 6880     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=9200, episode_reward=-71.69 +/- 144.29\n",
            "Episode length: 482.60 +/- 271.24\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 483      |\n",
            "|    mean_reward      | -71.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.126    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 9200     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.3      |\n",
            "|    n_updates        | 6905     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=9600, episode_reward=54.60 +/- 98.95\n",
            "Episode length: 606.40 +/- 327.19\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 606      |\n",
            "|    mean_reward      | 54.6     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.0884   |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 9600     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.39     |\n",
            "|    n_updates        | 6930     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=-41.63 +/- 138.46\n",
            "Episode length: 300.80 +/- 49.41\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 301      |\n",
            "|    mean_reward      | -41.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.0504   |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 10000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.03     |\n",
            "|    n_updates        | 6955     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=10400, episode_reward=16.89 +/- 78.25\n",
            "Episode length: 670.00 +/- 278.14\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 670      |\n",
            "|    mean_reward      | 16.9     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 10400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.11     |\n",
            "|    n_updates        | 6980     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 160      |\n",
            "|    ep_rew_mean      | -106     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 56       |\n",
            "|    fps              | 83       |\n",
            "|    time_elapsed     | 125      |\n",
            "|    total_timesteps  | 10468    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.25     |\n",
            "|    n_updates        | 6985     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=10800, episode_reward=5.27 +/- 141.64\n",
            "Episode length: 461.00 +/- 270.37\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 461      |\n",
            "|    mean_reward      | 5.27     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 10800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.793    |\n",
            "|    n_updates        | 7005     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=11200, episode_reward=110.73 +/- 103.00\n",
            "Episode length: 576.80 +/- 296.58\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 577      |\n",
            "|    mean_reward      | 111      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 11200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.23     |\n",
            "|    n_updates        | 7030     |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=11600, episode_reward=-38.46 +/- 153.06\n",
            "Episode length: 623.80 +/- 315.77\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 624      |\n",
            "|    mean_reward      | -38.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 11600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.15     |\n",
            "|    n_updates        | 7055     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 188      |\n",
            "|    ep_rew_mean      | -104     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 60       |\n",
            "|    fps              | 83       |\n",
            "|    time_elapsed     | 141      |\n",
            "|    total_timesteps  | 11844    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.11     |\n",
            "|    n_updates        | 7071     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=12000, episode_reward=93.36 +/- 155.07\n",
            "Episode length: 779.60 +/- 281.99\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 780      |\n",
            "|    mean_reward      | 93.4     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 12000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.686    |\n",
            "|    n_updates        | 7080     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=12400, episode_reward=-5.92 +/- 135.25\n",
            "Episode length: 432.00 +/- 73.22\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 432      |\n",
            "|    mean_reward      | -5.92    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 12400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.968    |\n",
            "|    n_updates        | 7105     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=12800, episode_reward=70.27 +/- 62.96\n",
            "Episode length: 758.40 +/- 304.86\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 758      |\n",
            "|    mean_reward      | 70.3     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 12800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.22     |\n",
            "|    n_updates        | 7130     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 198      |\n",
            "|    ep_rew_mean      | -94.5    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 64       |\n",
            "|    fps              | 79       |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 13048    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.956    |\n",
            "|    n_updates        | 7146     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=13200, episode_reward=88.70 +/- 156.11\n",
            "Episode length: 633.20 +/- 214.32\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 633      |\n",
            "|    mean_reward      | 88.7     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 13200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.87     |\n",
            "|    n_updates        | 7155     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=13600, episode_reward=48.95 +/- 183.22\n",
            "Episode length: 499.40 +/- 251.91\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 499      |\n",
            "|    mean_reward      | 49       |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 13600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.45     |\n",
            "|    n_updates        | 7180     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=14000, episode_reward=25.25 +/- 105.82\n",
            "Episode length: 671.20 +/- 294.39\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 671      |\n",
            "|    mean_reward      | 25.3     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 14000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.977    |\n",
            "|    n_updates        | 7205     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=14400, episode_reward=0.90 +/- 173.40\n",
            "Episode length: 414.20 +/- 71.98\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 414      |\n",
            "|    mean_reward      | 0.904    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 14400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.916    |\n",
            "|    n_updates        | 7230     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 208      |\n",
            "|    ep_rew_mean      | -89.9    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 68       |\n",
            "|    fps              | 80       |\n",
            "|    time_elapsed     | 180      |\n",
            "|    total_timesteps  | 14508    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.13     |\n",
            "|    n_updates        | 7237     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=14800, episode_reward=77.13 +/- 91.45\n",
            "Episode length: 504.00 +/- 250.50\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 504      |\n",
            "|    mean_reward      | 77.1     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 14800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.02     |\n",
            "|    n_updates        | 7255     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=15200, episode_reward=46.67 +/- 131.04\n",
            "Episode length: 748.80 +/- 311.39\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 749      |\n",
            "|    mean_reward      | 46.7     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 15200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.554    |\n",
            "|    n_updates        | 7280     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=15600, episode_reward=35.14 +/- 203.56\n",
            "Episode length: 413.00 +/- 33.95\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 413      |\n",
            "|    mean_reward      | 35.1     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 15600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.945    |\n",
            "|    n_updates        | 7305     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=112.58 +/- 118.78\n",
            "Episode length: 655.60 +/- 281.67\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 656      |\n",
            "|    mean_reward      | 113      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 16000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.07     |\n",
            "|    n_updates        | 7330     |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=16400, episode_reward=6.34 +/- 79.33\n",
            "Episode length: 369.60 +/- 77.70\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 370      |\n",
            "|    mean_reward      | 6.34     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 16400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.99     |\n",
            "|    n_updates        | 7355     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=16800, episode_reward=122.05 +/- 76.14\n",
            "Episode length: 474.40 +/- 185.01\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 474      |\n",
            "|    mean_reward      | 122      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 16800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.83     |\n",
            "|    n_updates        | 7380     |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 221      |\n",
            "|    ep_rew_mean      | -81.8    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 72       |\n",
            "|    fps              | 80       |\n",
            "|    time_elapsed     | 210      |\n",
            "|    total_timesteps  | 16964    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.61     |\n",
            "|    n_updates        | 7391     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=17200, episode_reward=41.68 +/- 145.03\n",
            "Episode length: 672.40 +/- 200.44\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 672      |\n",
            "|    mean_reward      | 41.7     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 17200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.09     |\n",
            "|    n_updates        | 7405     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=17600, episode_reward=67.58 +/- 102.26\n",
            "Episode length: 564.00 +/- 236.48\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 564      |\n",
            "|    mean_reward      | 67.6     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 17600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.05     |\n",
            "|    n_updates        | 7430     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=18000, episode_reward=151.77 +/- 61.82\n",
            "Episode length: 614.80 +/- 201.16\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 615      |\n",
            "|    mean_reward      | 152      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 18000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.823    |\n",
            "|    n_updates        | 7455     |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=18400, episode_reward=24.87 +/- 181.39\n",
            "Episode length: 494.80 +/- 256.30\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 495      |\n",
            "|    mean_reward      | 24.9     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 18400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.736    |\n",
            "|    n_updates        | 7480     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=18800, episode_reward=26.82 +/- 180.77\n",
            "Episode length: 644.80 +/- 305.18\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 645      |\n",
            "|    mean_reward      | 26.8     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 18800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.35     |\n",
            "|    n_updates        | 7505     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=19200, episode_reward=-32.73 +/- 172.22\n",
            "Episode length: 536.80 +/- 248.13\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 537      |\n",
            "|    mean_reward      | -32.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 19200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.66     |\n",
            "|    n_updates        | 7530     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=19600, episode_reward=40.03 +/- 162.40\n",
            "Episode length: 528.60 +/- 264.56\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 529      |\n",
            "|    mean_reward      | 40       |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 19600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.24     |\n",
            "|    n_updates        | 7555     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 244      |\n",
            "|    ep_rew_mean      | -69      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 76       |\n",
            "|    fps              | 80       |\n",
            "|    time_elapsed     | 246      |\n",
            "|    total_timesteps  | 19788    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.57     |\n",
            "|    n_updates        | 7567     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=-63.10 +/- 84.54\n",
            "Episode length: 656.80 +/- 283.41\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 657      |\n",
            "|    mean_reward      | -63.1    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 20000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.44     |\n",
            "|    n_updates        | 7580     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=20400, episode_reward=94.32 +/- 31.74\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | 94.3     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 20400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.49     |\n",
            "|    n_updates        | 7605     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=20800, episode_reward=56.51 +/- 108.62\n",
            "Episode length: 803.20 +/- 241.04\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 803      |\n",
            "|    mean_reward      | 56.5     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 20800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.22     |\n",
            "|    n_updates        | 7630     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=21200, episode_reward=29.02 +/- 140.53\n",
            "Episode length: 807.00 +/- 219.78\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 807      |\n",
            "|    mean_reward      | 29       |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 21200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.14     |\n",
            "|    n_updates        | 7655     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=21600, episode_reward=-12.68 +/- 94.57\n",
            "Episode length: 697.20 +/- 313.51\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 697      |\n",
            "|    mean_reward      | -12.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 21600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.838    |\n",
            "|    n_updates        | 7680     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=22000, episode_reward=84.33 +/- 100.07\n",
            "Episode length: 584.40 +/- 238.41\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 584      |\n",
            "|    mean_reward      | 84.3     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 22000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.43     |\n",
            "|    n_updates        | 7705     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 259      |\n",
            "|    ep_rew_mean      | -64.4    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 80       |\n",
            "|    fps              | 76       |\n",
            "|    time_elapsed     | 288      |\n",
            "|    total_timesteps  | 22032    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.46     |\n",
            "|    n_updates        | 7707     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=22400, episode_reward=2.35 +/- 105.25\n",
            "Episode length: 497.00 +/- 264.37\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 497      |\n",
            "|    mean_reward      | 2.35     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 22400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.882    |\n",
            "|    n_updates        | 7730     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=22800, episode_reward=-23.24 +/- 142.98\n",
            "Episode length: 358.20 +/- 78.36\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 358      |\n",
            "|    mean_reward      | -23.2    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 22800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.835    |\n",
            "|    n_updates        | 7755     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=23200, episode_reward=-37.04 +/- 129.79\n",
            "Episode length: 368.20 +/- 58.59\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 368      |\n",
            "|    mean_reward      | -37      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 23200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.979    |\n",
            "|    n_updates        | 7780     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=23600, episode_reward=76.86 +/- 147.97\n",
            "Episode length: 583.60 +/- 216.13\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 584      |\n",
            "|    mean_reward      | 76.9     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 23600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.43     |\n",
            "|    n_updates        | 7805     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=-10.42 +/- 111.57\n",
            "Episode length: 545.60 +/- 238.10\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 546      |\n",
            "|    mean_reward      | -10.4    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 24000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.881    |\n",
            "|    n_updates        | 7830     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=24400, episode_reward=102.86 +/- 88.77\n",
            "Episode length: 573.40 +/- 240.70\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 573      |\n",
            "|    mean_reward      | 103      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 24400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.775    |\n",
            "|    n_updates        | 7855     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=24800, episode_reward=115.78 +/- 29.94\n",
            "Episode length: 735.80 +/- 229.30\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 736      |\n",
            "|    mean_reward      | 116      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 24800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.973    |\n",
            "|    n_updates        | 7880     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=25200, episode_reward=8.72 +/- 126.18\n",
            "Episode length: 933.60 +/- 132.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 934      |\n",
            "|    mean_reward      | 8.72     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 25200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.71     |\n",
            "|    n_updates        | 7905     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 290      |\n",
            "|    ep_rew_mean      | -61.3    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 84       |\n",
            "|    fps              | 75       |\n",
            "|    time_elapsed     | 336      |\n",
            "|    total_timesteps  | 25556    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.74     |\n",
            "|    n_updates        | 7928     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=25600, episode_reward=-3.64 +/- 100.11\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -3.64    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 25600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.672    |\n",
            "|    n_updates        | 7930     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=26000, episode_reward=45.87 +/- 132.42\n",
            "Episode length: 644.20 +/- 293.26\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 644      |\n",
            "|    mean_reward      | 45.9     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 26000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.843    |\n",
            "|    n_updates        | 7955     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=26400, episode_reward=95.97 +/- 63.27\n",
            "Episode length: 755.60 +/- 303.90\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 756      |\n",
            "|    mean_reward      | 96       |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 26400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.33     |\n",
            "|    n_updates        | 7980     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=26800, episode_reward=-55.25 +/- 64.62\n",
            "Episode length: 394.20 +/- 43.01\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 394      |\n",
            "|    mean_reward      | -55.3    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 26800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.16     |\n",
            "|    n_updates        | 8005     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=27200, episode_reward=58.65 +/- 101.15\n",
            "Episode length: 446.00 +/- 105.38\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 446      |\n",
            "|    mean_reward      | 58.6     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 27200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.01     |\n",
            "|    n_updates        | 8030     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=27600, episode_reward=81.32 +/- 104.69\n",
            "Episode length: 574.20 +/- 255.02\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 574      |\n",
            "|    mean_reward      | 81.3     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 27600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.86     |\n",
            "|    n_updates        | 8055     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=28000, episode_reward=99.93 +/- 128.89\n",
            "Episode length: 575.40 +/- 125.98\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 575      |\n",
            "|    mean_reward      | 99.9     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 28000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.55     |\n",
            "|    n_updates        | 8080     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 311      |\n",
            "|    ep_rew_mean      | -55.8    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 88       |\n",
            "|    fps              | 75       |\n",
            "|    time_elapsed     | 374      |\n",
            "|    total_timesteps  | 28388    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.35     |\n",
            "|    n_updates        | 8105     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=28400, episode_reward=88.30 +/- 53.39\n",
            "Episode length: 846.40 +/- 194.32\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 846      |\n",
            "|    mean_reward      | 88.3     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 28400    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=28800, episode_reward=15.14 +/- 139.19\n",
            "Episode length: 696.60 +/- 175.15\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 697      |\n",
            "|    mean_reward      | 15.1     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 28800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.25     |\n",
            "|    n_updates        | 8130     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=29200, episode_reward=79.92 +/- 87.09\n",
            "Episode length: 740.20 +/- 139.70\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 740      |\n",
            "|    mean_reward      | 79.9     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 29200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.4      |\n",
            "|    n_updates        | 8155     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=29600, episode_reward=-39.35 +/- 112.08\n",
            "Episode length: 972.20 +/- 55.60\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 972      |\n",
            "|    mean_reward      | -39.3    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 29600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.83     |\n",
            "|    n_updates        | 8180     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=6.54 +/- 118.63\n",
            "Episode length: 808.60 +/- 247.29\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 809      |\n",
            "|    mean_reward      | 6.54     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 30000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.852    |\n",
            "|    n_updates        | 8205     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30400, episode_reward=24.13 +/- 148.50\n",
            "Episode length: 732.20 +/- 287.01\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 732      |\n",
            "|    mean_reward      | 24.1     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 30400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.88     |\n",
            "|    n_updates        | 8230     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30800, episode_reward=-42.21 +/- 112.88\n",
            "Episode length: 858.80 +/- 191.21\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 859      |\n",
            "|    mean_reward      | -42.2    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 30800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.611    |\n",
            "|    n_updates        | 8255     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=31200, episode_reward=-56.73 +/- 81.79\n",
            "Episode length: 696.40 +/- 257.97\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 696      |\n",
            "|    mean_reward      | -56.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 31200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.04     |\n",
            "|    n_updates        | 8280     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 332      |\n",
            "|    ep_rew_mean      | -53.3    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 92       |\n",
            "|    fps              | 71       |\n",
            "|    time_elapsed     | 442      |\n",
            "|    total_timesteps  | 31548    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.23     |\n",
            "|    n_updates        | 8302     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=31600, episode_reward=-151.11 +/- 24.49\n",
            "Episode length: 895.20 +/- 128.66\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 895      |\n",
            "|    mean_reward      | -151     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 31600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.598    |\n",
            "|    n_updates        | 8305     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=-97.38 +/- 80.21\n",
            "Episode length: 833.20 +/- 213.63\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 833      |\n",
            "|    mean_reward      | -97.4    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 32000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.985    |\n",
            "|    n_updates        | 8330     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=32400, episode_reward=-97.42 +/- 127.08\n",
            "Episode length: 771.20 +/- 197.47\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 771      |\n",
            "|    mean_reward      | -97.4    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 32400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.737    |\n",
            "|    n_updates        | 8355     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=32800, episode_reward=-48.14 +/- 114.19\n",
            "Episode length: 849.40 +/- 185.52\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 849      |\n",
            "|    mean_reward      | -48.1    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 32800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.49     |\n",
            "|    n_updates        | 8380     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=33200, episode_reward=-63.96 +/- 140.96\n",
            "Episode length: 917.40 +/- 143.31\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 917      |\n",
            "|    mean_reward      | -64      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 33200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.27     |\n",
            "|    n_updates        | 8405     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=33600, episode_reward=26.44 +/- 142.90\n",
            "Episode length: 633.40 +/- 269.58\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 633      |\n",
            "|    mean_reward      | 26.4     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 33600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.68     |\n",
            "|    n_updates        | 8430     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 340      |\n",
            "|    ep_rew_mean      | -51      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 96       |\n",
            "|    fps              | 67       |\n",
            "|    time_elapsed     | 500      |\n",
            "|    total_timesteps  | 33724    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.32     |\n",
            "|    n_updates        | 8438     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=34000, episode_reward=-83.54 +/- 90.62\n",
            "Episode length: 771.20 +/- 213.34\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 771      |\n",
            "|    mean_reward      | -83.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 34000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.04     |\n",
            "|    n_updates        | 8455     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=34400, episode_reward=-102.18 +/- 158.31\n",
            "Episode length: 753.20 +/- 179.90\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 753      |\n",
            "|    mean_reward      | -102     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 34400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.47     |\n",
            "|    n_updates        | 8480     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=34800, episode_reward=-45.27 +/- 106.98\n",
            "Episode length: 852.00 +/- 186.01\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 852      |\n",
            "|    mean_reward      | -45.3    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 34800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.7      |\n",
            "|    n_updates        | 8505     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=35200, episode_reward=-101.85 +/- 65.19\n",
            "Episode length: 960.80 +/- 78.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 961      |\n",
            "|    mean_reward      | -102     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 35200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.612    |\n",
            "|    n_updates        | 8530     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=35600, episode_reward=-95.27 +/- 141.97\n",
            "Episode length: 786.80 +/- 160.51\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 787      |\n",
            "|    mean_reward      | -95.3    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 35600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.52     |\n",
            "|    n_updates        | 8555     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=36000, episode_reward=-20.17 +/- 112.50\n",
            "Episode length: 787.20 +/- 283.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 787      |\n",
            "|    mean_reward      | -20.2    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 36000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.02     |\n",
            "|    n_updates        | 8580     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=36400, episode_reward=7.87 +/- 94.31\n",
            "Episode length: 890.60 +/- 168.10\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 891      |\n",
            "|    mean_reward      | 7.87     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 36400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.958    |\n",
            "|    n_updates        | 8605     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=36800, episode_reward=18.60 +/- 104.00\n",
            "Episode length: 805.60 +/- 166.69\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 806      |\n",
            "|    mean_reward      | 18.6     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 36800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.607    |\n",
            "|    n_updates        | 8630     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 358      |\n",
            "|    ep_rew_mean      | -52.1    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 100      |\n",
            "|    fps              | 64       |\n",
            "|    time_elapsed     | 570      |\n",
            "|    total_timesteps  | 37004    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.59     |\n",
            "|    n_updates        | 8643     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=37200, episode_reward=18.05 +/- 106.42\n",
            "Episode length: 576.00 +/- 218.53\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 576      |\n",
            "|    mean_reward      | 18.1     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 37200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.938    |\n",
            "|    n_updates        | 8655     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=37600, episode_reward=-33.74 +/- 92.70\n",
            "Episode length: 578.20 +/- 233.54\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 578      |\n",
            "|    mean_reward      | -33.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 37600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.23     |\n",
            "|    n_updates        | 8680     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=38000, episode_reward=125.38 +/- 46.18\n",
            "Episode length: 793.20 +/- 253.66\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 793      |\n",
            "|    mean_reward      | 125      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 38000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.678    |\n",
            "|    n_updates        | 8705     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=38400, episode_reward=-99.13 +/- 104.82\n",
            "Episode length: 920.20 +/- 117.98\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 920      |\n",
            "|    mean_reward      | -99.1    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 38400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.06     |\n",
            "|    n_updates        | 8730     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=38800, episode_reward=-25.00 +/- 154.19\n",
            "Episode length: 752.80 +/- 259.26\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 753      |\n",
            "|    mean_reward      | -25      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 38800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.76     |\n",
            "|    n_updates        | 8755     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 376      |\n",
            "|    ep_rew_mean      | -50.4    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 104      |\n",
            "|    fps              | 63       |\n",
            "|    time_elapsed     | 612      |\n",
            "|    total_timesteps  | 39156    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.34     |\n",
            "|    n_updates        | 8778     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=39200, episode_reward=49.75 +/- 131.71\n",
            "Episode length: 517.80 +/- 250.05\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 518      |\n",
            "|    mean_reward      | 49.8     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 39200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.31     |\n",
            "|    n_updates        | 8780     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=39600, episode_reward=-109.26 +/- 80.65\n",
            "Episode length: 870.20 +/- 217.41\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 870      |\n",
            "|    mean_reward      | -109     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 39600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.45     |\n",
            "|    n_updates        | 8805     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=24.91 +/- 163.29\n",
            "Episode length: 663.00 +/- 278.35\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 663      |\n",
            "|    mean_reward      | 24.9     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 40000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.994    |\n",
            "|    n_updates        | 8830     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=40400, episode_reward=-59.00 +/- 138.03\n",
            "Episode length: 820.00 +/- 223.08\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 820      |\n",
            "|    mean_reward      | -59      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 40400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.1      |\n",
            "|    n_updates        | 8855     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=40800, episode_reward=-84.62 +/- 136.80\n",
            "Episode length: 726.40 +/- 214.48\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 726      |\n",
            "|    mean_reward      | -84.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 40800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.32     |\n",
            "|    n_updates        | 8880     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=41200, episode_reward=-9.67 +/- 138.36\n",
            "Episode length: 692.80 +/- 173.64\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 693      |\n",
            "|    mean_reward      | -9.67    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 41200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.2      |\n",
            "|    n_updates        | 8905     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=41600, episode_reward=59.42 +/- 172.40\n",
            "Episode length: 765.40 +/- 224.84\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 765      |\n",
            "|    mean_reward      | 59.4     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 41600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.82     |\n",
            "|    n_updates        | 8930     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=42000, episode_reward=-93.84 +/- 128.77\n",
            "Episode length: 780.80 +/- 229.63\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 781      |\n",
            "|    mean_reward      | -93.8    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 42000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.41     |\n",
            "|    n_updates        | 8955     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 395      |\n",
            "|    ep_rew_mean      | -43.6    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 108      |\n",
            "|    fps              | 62       |\n",
            "|    time_elapsed     | 680      |\n",
            "|    total_timesteps  | 42224    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.08     |\n",
            "|    n_updates        | 8969     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=42400, episode_reward=-92.93 +/- 103.04\n",
            "Episode length: 982.00 +/- 36.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 982      |\n",
            "|    mean_reward      | -92.9    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 42400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.734    |\n",
            "|    n_updates        | 8980     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=42800, episode_reward=39.37 +/- 141.70\n",
            "Episode length: 681.80 +/- 273.36\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 682      |\n",
            "|    mean_reward      | 39.4     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 42800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.841    |\n",
            "|    n_updates        | 9005     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=43200, episode_reward=-50.38 +/- 118.86\n",
            "Episode length: 745.00 +/- 225.19\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 745      |\n",
            "|    mean_reward      | -50.4    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 43200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.17     |\n",
            "|    n_updates        | 9030     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=43600, episode_reward=-26.41 +/- 152.52\n",
            "Episode length: 774.00 +/- 276.83\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 774      |\n",
            "|    mean_reward      | -26.4    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 43600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.03     |\n",
            "|    n_updates        | 9055     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=44000, episode_reward=-114.36 +/- 13.23\n",
            "Episode length: 949.00 +/- 102.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 949      |\n",
            "|    mean_reward      | -114     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 44000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.08     |\n",
            "|    n_updates        | 9080     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 424      |\n",
            "|    ep_rew_mean      | -38.4    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 112      |\n",
            "|    fps              | 60       |\n",
            "|    time_elapsed     | 729      |\n",
            "|    total_timesteps  | 44388    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.883    |\n",
            "|    n_updates        | 9105     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=44400, episode_reward=-130.67 +/- 50.03\n",
            "Episode length: 866.80 +/- 244.99\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 867      |\n",
            "|    mean_reward      | -131     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 44400    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=44800, episode_reward=-147.40 +/- 45.02\n",
            "Episode length: 972.60 +/- 33.68\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 973      |\n",
            "|    mean_reward      | -147     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 44800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.08     |\n",
            "|    n_updates        | 9130     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=45200, episode_reward=-135.24 +/- 50.07\n",
            "Episode length: 954.80 +/- 81.70\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 955      |\n",
            "|    mean_reward      | -135     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 45200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.43     |\n",
            "|    n_updates        | 9155     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=45600, episode_reward=-64.77 +/- 106.83\n",
            "Episode length: 673.00 +/- 279.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 673      |\n",
            "|    mean_reward      | -64.8    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 45600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.86     |\n",
            "|    n_updates        | 9180     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=46000, episode_reward=-61.69 +/- 68.15\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -61.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 46000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.793    |\n",
            "|    n_updates        | 9205     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=46400, episode_reward=-81.93 +/- 129.61\n",
            "Episode length: 706.40 +/- 242.95\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 706      |\n",
            "|    mean_reward      | -81.9    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 46400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.89     |\n",
            "|    n_updates        | 9230     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=46800, episode_reward=-123.27 +/- 61.46\n",
            "Episode length: 849.20 +/- 252.83\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 849      |\n",
            "|    mean_reward      | -123     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 46800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.31     |\n",
            "|    n_updates        | 9255     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=47200, episode_reward=-72.75 +/- 76.19\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -72.8    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 47200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.816    |\n",
            "|    n_updates        | 9280     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | -40.3    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 116      |\n",
            "|    fps              | 58       |\n",
            "|    time_elapsed     | 813      |\n",
            "|    total_timesteps  | 47264    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.03     |\n",
            "|    n_updates        | 9284     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=47600, episode_reward=-142.76 +/- 17.86\n",
            "Episode length: 905.40 +/- 153.21\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 905      |\n",
            "|    mean_reward      | -143     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 47600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.13     |\n",
            "|    n_updates        | 9305     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=-178.04 +/- 33.98\n",
            "Episode length: 836.80 +/- 83.48\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 837      |\n",
            "|    mean_reward      | -178     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 48000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.478    |\n",
            "|    n_updates        | 9330     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=48400, episode_reward=-114.14 +/- 114.01\n",
            "Episode length: 918.60 +/- 117.56\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 919      |\n",
            "|    mean_reward      | -114     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 48400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.24     |\n",
            "|    n_updates        | 9355     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=48800, episode_reward=-105.34 +/- 14.99\n",
            "Episode length: 667.00 +/- 275.30\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 667      |\n",
            "|    mean_reward      | -105     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 48800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.05     |\n",
            "|    n_updates        | 9380     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=49200, episode_reward=-91.61 +/- 81.05\n",
            "Episode length: 890.40 +/- 219.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 890      |\n",
            "|    mean_reward      | -91.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 49200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.13     |\n",
            "|    n_updates        | 9405     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=49600, episode_reward=26.00 +/- 149.61\n",
            "Episode length: 795.20 +/- 173.26\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 795      |\n",
            "|    mean_reward      | 26       |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 49600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.31     |\n",
            "|    n_updates        | 9430     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=-14.20 +/- 126.48\n",
            "Episode length: 877.80 +/- 244.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 878      |\n",
            "|    mean_reward      | -14.2    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 50000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.16     |\n",
            "|    n_updates        | 9455     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50400, episode_reward=-145.49 +/- 44.50\n",
            "Episode length: 799.00 +/- 168.68\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 799      |\n",
            "|    mean_reward      | -145     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 50400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.57     |\n",
            "|    n_updates        | 9480     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 473      |\n",
            "|    ep_rew_mean      | -37.4    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 120      |\n",
            "|    fps              | 56       |\n",
            "|    time_elapsed     | 898      |\n",
            "|    total_timesteps  | 50404    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.23     |\n",
            "|    n_updates        | 9481     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50800, episode_reward=-95.12 +/- 18.30\n",
            "Episode length: 894.40 +/- 211.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 894      |\n",
            "|    mean_reward      | -95.1    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 50800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.37     |\n",
            "|    n_updates        | 9505     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=51200, episode_reward=-139.14 +/- 38.49\n",
            "Episode length: 877.80 +/- 168.24\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 878      |\n",
            "|    mean_reward      | -139     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 51200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.04     |\n",
            "|    n_updates        | 9530     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=51600, episode_reward=-100.15 +/- 27.59\n",
            "Episode length: 902.60 +/- 194.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 903      |\n",
            "|    mean_reward      | -100     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 51600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.06     |\n",
            "|    n_updates        | 9555     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=52000, episode_reward=-45.74 +/- 91.95\n",
            "Episode length: 929.60 +/- 140.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 930      |\n",
            "|    mean_reward      | -45.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 52000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.32     |\n",
            "|    n_updates        | 9580     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=52400, episode_reward=-100.37 +/- 39.47\n",
            "Episode length: 766.80 +/- 207.71\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 767      |\n",
            "|    mean_reward      | -100     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 52400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.953    |\n",
            "|    n_updates        | 9605     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=52800, episode_reward=-61.97 +/- 73.43\n",
            "Episode length: 882.60 +/- 234.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 883      |\n",
            "|    mean_reward      | -62      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 52800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.04     |\n",
            "|    n_updates        | 9630     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 502      |\n",
            "|    ep_rew_mean      | -34.5    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 124      |\n",
            "|    fps              | 54       |\n",
            "|    time_elapsed     | 965      |\n",
            "|    total_timesteps  | 52988    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.66     |\n",
            "|    n_updates        | 9642     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=53200, episode_reward=-114.83 +/- 46.70\n",
            "Episode length: 969.20 +/- 61.60\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 969      |\n",
            "|    mean_reward      | -115     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 53200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.48     |\n",
            "|    n_updates        | 9655     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=53600, episode_reward=-124.75 +/- 50.71\n",
            "Episode length: 790.60 +/- 229.23\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 791      |\n",
            "|    mean_reward      | -125     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 53600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.06     |\n",
            "|    n_updates        | 9680     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=54000, episode_reward=-108.26 +/- 19.29\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -108     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 54000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.27     |\n",
            "|    n_updates        | 9705     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=54400, episode_reward=-100.27 +/- 27.61\n",
            "Episode length: 958.40 +/- 83.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 958      |\n",
            "|    mean_reward      | -100     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 54400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.761    |\n",
            "|    n_updates        | 9730     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=54800, episode_reward=-80.96 +/- 61.49\n",
            "Episode length: 867.80 +/- 264.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 868      |\n",
            "|    mean_reward      | -81      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 54800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.718    |\n",
            "|    n_updates        | 9755     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=55200, episode_reward=-99.50 +/- 145.88\n",
            "Episode length: 657.80 +/- 210.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 658      |\n",
            "|    mean_reward      | -99.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 55200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.1      |\n",
            "|    n_updates        | 9780     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=55600, episode_reward=-61.07 +/- 109.11\n",
            "Episode length: 796.80 +/- 252.24\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 797      |\n",
            "|    mean_reward      | -61.1    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 55600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.3      |\n",
            "|    n_updates        | 9805     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=56000, episode_reward=-110.90 +/- 18.10\n",
            "Episode length: 868.80 +/- 166.84\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 869      |\n",
            "|    mean_reward      | -111     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 56000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.1      |\n",
            "|    n_updates        | 9830     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=56400, episode_reward=-108.03 +/- 55.87\n",
            "Episode length: 743.00 +/- 228.05\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 743      |\n",
            "|    mean_reward      | -108     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 56400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.59     |\n",
            "|    n_updates        | 9855     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=56800, episode_reward=-106.30 +/- 10.70\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -106     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 56800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.6      |\n",
            "|    n_updates        | 9880     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 528      |\n",
            "|    ep_rew_mean      | -31.2    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 128      |\n",
            "|    fps              | 53       |\n",
            "|    time_elapsed     | 1067     |\n",
            "|    total_timesteps  | 56848    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.36     |\n",
            "|    n_updates        | 9883     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=57200, episode_reward=-123.98 +/- 45.85\n",
            "Episode length: 997.80 +/- 4.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 998      |\n",
            "|    mean_reward      | -124     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 57200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.735    |\n",
            "|    n_updates        | 9905     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=57600, episode_reward=-81.96 +/- 35.23\n",
            "Episode length: 931.20 +/- 137.60\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 931      |\n",
            "|    mean_reward      | -82      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 57600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.955    |\n",
            "|    n_updates        | 9930     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=58000, episode_reward=-108.56 +/- 28.41\n",
            "Episode length: 916.40 +/- 167.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 916      |\n",
            "|    mean_reward      | -109     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 58000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.01     |\n",
            "|    n_updates        | 9955     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=58400, episode_reward=-81.32 +/- 112.86\n",
            "Episode length: 579.40 +/- 217.98\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 579      |\n",
            "|    mean_reward      | -81.3    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 58400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.775    |\n",
            "|    n_updates        | 9980     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=58800, episode_reward=16.23 +/- 78.33\n",
            "Episode length: 981.20 +/- 37.60\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 981      |\n",
            "|    mean_reward      | 16.2     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 58800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.822    |\n",
            "|    n_updates        | 10005    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=59200, episode_reward=-79.54 +/- 151.21\n",
            "Episode length: 683.00 +/- 254.47\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 683      |\n",
            "|    mean_reward      | -79.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 59200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.933    |\n",
            "|    n_updates        | 10030    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 548      |\n",
            "|    ep_rew_mean      | -28      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 132      |\n",
            "|    fps              | 52       |\n",
            "|    time_elapsed     | 1124     |\n",
            "|    total_timesteps  | 59264    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.31     |\n",
            "|    n_updates        | 10034    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=59600, episode_reward=-94.97 +/- 25.98\n",
            "Episode length: 770.80 +/- 286.30\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 771      |\n",
            "|    mean_reward      | -95      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 59600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.959    |\n",
            "|    n_updates        | 10055    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=-97.98 +/- 23.85\n",
            "Episode length: 885.20 +/- 229.60\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 885      |\n",
            "|    mean_reward      | -98      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 60000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.99     |\n",
            "|    n_updates        | 10080    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60400, episode_reward=-93.10 +/- 20.75\n",
            "Episode length: 814.00 +/- 228.03\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 814      |\n",
            "|    mean_reward      | -93.1    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 60400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.732    |\n",
            "|    n_updates        | 10105    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60800, episode_reward=-72.59 +/- 131.79\n",
            "Episode length: 866.00 +/- 128.32\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 866      |\n",
            "|    mean_reward      | -72.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 60800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.844    |\n",
            "|    n_updates        | 10130    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=61200, episode_reward=-79.62 +/- 20.56\n",
            "Episode length: 893.00 +/- 214.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 893      |\n",
            "|    mean_reward      | -79.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 61200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.21     |\n",
            "|    n_updates        | 10155    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=61600, episode_reward=-94.58 +/- 19.87\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -94.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 61600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.36     |\n",
            "|    n_updates        | 10180    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=62000, episode_reward=-125.99 +/- 43.21\n",
            "Episode length: 987.00 +/- 26.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 987      |\n",
            "|    mean_reward      | -126     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 62000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2        |\n",
            "|    n_updates        | 10205    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=62400, episode_reward=-102.83 +/- 24.74\n",
            "Episode length: 938.60 +/- 122.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 939      |\n",
            "|    mean_reward      | -103     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 62400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.22     |\n",
            "|    n_updates        | 10230    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 579      |\n",
            "|    ep_rew_mean      | -27.4    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 136      |\n",
            "|    fps              | 51       |\n",
            "|    time_elapsed     | 1211     |\n",
            "|    total_timesteps  | 62780    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.37     |\n",
            "|    n_updates        | 10254    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=62800, episode_reward=-84.67 +/- 18.80\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -84.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 62800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.804    |\n",
            "|    n_updates        | 10255    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=63200, episode_reward=-120.34 +/- 60.69\n",
            "Episode length: 962.20 +/- 73.62\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 962      |\n",
            "|    mean_reward      | -120     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 63200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.2      |\n",
            "|    n_updates        | 10280    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=63600, episode_reward=-80.94 +/- 11.33\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -80.9    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 63600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1        |\n",
            "|    n_updates        | 10305    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=64000, episode_reward=-112.39 +/- 35.28\n",
            "Episode length: 971.60 +/- 56.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 972      |\n",
            "|    mean_reward      | -112     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 64000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.731    |\n",
            "|    n_updates        | 10330    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=64400, episode_reward=-121.72 +/- 51.06\n",
            "Episode length: 946.20 +/- 100.27\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 946      |\n",
            "|    mean_reward      | -122     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 64400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.07     |\n",
            "|    n_updates        | 10355    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=64800, episode_reward=-112.87 +/- 38.14\n",
            "Episode length: 922.60 +/- 95.07\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 923      |\n",
            "|    mean_reward      | -113     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 64800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.11     |\n",
            "|    n_updates        | 10380    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=65200, episode_reward=-104.56 +/- 29.74\n",
            "Episode length: 944.20 +/- 111.60\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 944      |\n",
            "|    mean_reward      | -105     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 65200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.661    |\n",
            "|    n_updates        | 10405    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=65600, episode_reward=-112.89 +/- 28.53\n",
            "Episode length: 974.20 +/- 51.60\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 974      |\n",
            "|    mean_reward      | -113     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 65600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.12     |\n",
            "|    n_updates        | 10430    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=66000, episode_reward=-97.75 +/- 26.93\n",
            "Episode length: 910.20 +/- 179.60\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 910      |\n",
            "|    mean_reward      | -97.8    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 66000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.72     |\n",
            "|    n_updates        | 10455    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=66400, episode_reward=-108.26 +/- 15.03\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -108     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 66400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.87     |\n",
            "|    n_updates        | 10480    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 614      |\n",
            "|    ep_rew_mean      | -29.6    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 140      |\n",
            "|    fps              | 50       |\n",
            "|    time_elapsed     | 1335     |\n",
            "|    total_timesteps  | 66780    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.585    |\n",
            "|    n_updates        | 10504    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=66800, episode_reward=-102.92 +/- 13.75\n",
            "Episode length: 892.60 +/- 214.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 893      |\n",
            "|    mean_reward      | -103     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 66800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.665    |\n",
            "|    n_updates        | 10505    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=67200, episode_reward=-128.13 +/- 40.91\n",
            "Episode length: 975.20 +/- 49.60\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 975      |\n",
            "|    mean_reward      | -128     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 67200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.1      |\n",
            "|    n_updates        | 10530    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=67600, episode_reward=-20.25 +/- 115.47\n",
            "Episode length: 797.60 +/- 249.05\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 798      |\n",
            "|    mean_reward      | -20.2    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 67600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.58     |\n",
            "|    n_updates        | 10555    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=68000, episode_reward=-122.40 +/- 36.75\n",
            "Episode length: 816.80 +/- 200.48\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 817      |\n",
            "|    mean_reward      | -122     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 68000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.14     |\n",
            "|    n_updates        | 10580    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=68400, episode_reward=-101.22 +/- 30.17\n",
            "Episode length: 956.00 +/- 88.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 956      |\n",
            "|    mean_reward      | -101     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 68400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.901    |\n",
            "|    n_updates        | 10605    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=68800, episode_reward=-79.99 +/- 21.33\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -80      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 68800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.29     |\n",
            "|    n_updates        | 10630    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=69200, episode_reward=-96.42 +/- 44.61\n",
            "Episode length: 986.40 +/- 27.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 986      |\n",
            "|    mean_reward      | -96.4    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 69200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.09     |\n",
            "|    n_updates        | 10655    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=69600, episode_reward=-118.70 +/- 38.18\n",
            "Episode length: 837.60 +/- 151.75\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 838      |\n",
            "|    mean_reward      | -119     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 69600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.69     |\n",
            "|    n_updates        | 10680    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=-117.25 +/- 59.87\n",
            "Episode length: 939.20 +/- 103.21\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 939      |\n",
            "|    mean_reward      | -117     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 70000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.17     |\n",
            "|    n_updates        | 10705    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70400, episode_reward=-90.69 +/- 27.82\n",
            "Episode length: 838.60 +/- 214.95\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 839      |\n",
            "|    mean_reward      | -90.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 70400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.74     |\n",
            "|    n_updates        | 10730    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 647      |\n",
            "|    ep_rew_mean      | -32.2    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 144      |\n",
            "|    fps              | 48       |\n",
            "|    time_elapsed     | 1445     |\n",
            "|    total_timesteps  | 70780    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.787    |\n",
            "|    n_updates        | 10754    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70800, episode_reward=-115.60 +/- 43.15\n",
            "Episode length: 996.00 +/- 8.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 996      |\n",
            "|    mean_reward      | -116     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 70800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.45     |\n",
            "|    n_updates        | 10755    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=71200, episode_reward=-93.75 +/- 28.15\n",
            "Episode length: 944.40 +/- 111.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 944      |\n",
            "|    mean_reward      | -93.8    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 71200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.605    |\n",
            "|    n_updates        | 10780    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=71600, episode_reward=-111.45 +/- 40.63\n",
            "Episode length: 988.40 +/- 23.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 988      |\n",
            "|    mean_reward      | -111     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 71600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.27     |\n",
            "|    n_updates        | 10805    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=72000, episode_reward=-125.59 +/- 54.14\n",
            "Episode length: 960.80 +/- 78.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 961      |\n",
            "|    mean_reward      | -126     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 72000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.89     |\n",
            "|    n_updates        | 10830    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=72400, episode_reward=-104.88 +/- 37.01\n",
            "Episode length: 802.40 +/- 253.74\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 802      |\n",
            "|    mean_reward      | -105     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 72400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.13     |\n",
            "|    n_updates        | 10855    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=72800, episode_reward=-110.96 +/- 36.71\n",
            "Episode length: 994.60 +/- 10.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 995      |\n",
            "|    mean_reward      | -111     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 72800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.746    |\n",
            "|    n_updates        | 10880    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=73200, episode_reward=-94.07 +/- 34.96\n",
            "Episode length: 987.80 +/- 24.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 988      |\n",
            "|    mean_reward      | -94.1    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 73200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.891    |\n",
            "|    n_updates        | 10905    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=73600, episode_reward=-111.52 +/- 17.02\n",
            "Episode length: 935.40 +/- 129.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 935      |\n",
            "|    mean_reward      | -112     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 73600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.927    |\n",
            "|    n_updates        | 10930    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 667      |\n",
            "|    ep_rew_mean      | -32.5    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 148      |\n",
            "|    fps              | 47       |\n",
            "|    time_elapsed     | 1535     |\n",
            "|    total_timesteps  | 73668    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.66     |\n",
            "|    n_updates        | 10935    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=74000, episode_reward=-88.52 +/- 20.08\n",
            "Episode length: 913.00 +/- 174.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 913      |\n",
            "|    mean_reward      | -88.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 74000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.16     |\n",
            "|    n_updates        | 10955    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=74400, episode_reward=-106.14 +/- 45.52\n",
            "Episode length: 998.40 +/- 3.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 998      |\n",
            "|    mean_reward      | -106     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 74400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.65     |\n",
            "|    n_updates        | 10980    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=74800, episode_reward=-98.94 +/- 34.69\n",
            "Episode length: 933.80 +/- 132.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 934      |\n",
            "|    mean_reward      | -98.9    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 74800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.1      |\n",
            "|    n_updates        | 11005    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=75200, episode_reward=-71.92 +/- 82.99\n",
            "Episode length: 913.00 +/- 174.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 913      |\n",
            "|    mean_reward      | -71.9    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 75200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.04     |\n",
            "|    n_updates        | 11030    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=75600, episode_reward=-126.52 +/- 62.79\n",
            "Episode length: 899.00 +/- 175.91\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 899      |\n",
            "|    mean_reward      | -127     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 75600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.42     |\n",
            "|    n_updates        | 11055    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=76000, episode_reward=-86.81 +/- 32.89\n",
            "Episode length: 780.00 +/- 270.75\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 780      |\n",
            "|    mean_reward      | -86.8    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 76000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.665    |\n",
            "|    n_updates        | 11080    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=76400, episode_reward=-40.12 +/- 113.97\n",
            "Episode length: 889.20 +/- 172.35\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 889      |\n",
            "|    mean_reward      | -40.1    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 76400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.604    |\n",
            "|    n_updates        | 11105    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=76800, episode_reward=-63.15 +/- 13.37\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -63.2    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 76800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.5      |\n",
            "|    n_updates        | 11130    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=77200, episode_reward=-64.42 +/- 22.68\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -64.4    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 77200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.87     |\n",
            "|    n_updates        | 11155    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=77600, episode_reward=-88.02 +/- 19.46\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -88      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 77600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.48     |\n",
            "|    n_updates        | 11180    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 696      |\n",
            "|    ep_rew_mean      | -28.6    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 152      |\n",
            "|    fps              | 47       |\n",
            "|    time_elapsed     | 1644     |\n",
            "|    total_timesteps  | 77668    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.483    |\n",
            "|    n_updates        | 11185    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=78000, episode_reward=-111.69 +/- 42.49\n",
            "Episode length: 997.60 +/- 4.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 998      |\n",
            "|    mean_reward      | -112     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 78000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.913    |\n",
            "|    n_updates        | 11205    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=78400, episode_reward=-98.75 +/- 43.57\n",
            "Episode length: 981.60 +/- 36.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 982      |\n",
            "|    mean_reward      | -98.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 78400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.14     |\n",
            "|    n_updates        | 11230    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=78800, episode_reward=-77.87 +/- 18.66\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -77.9    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 78800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.858    |\n",
            "|    n_updates        | 11255    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=79200, episode_reward=-97.07 +/- 44.14\n",
            "Episode length: 981.20 +/- 37.60\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 981      |\n",
            "|    mean_reward      | -97.1    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 79200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.01     |\n",
            "|    n_updates        | 11280    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=79600, episode_reward=-93.34 +/- 16.45\n",
            "Episode length: 894.60 +/- 210.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 895      |\n",
            "|    mean_reward      | -93.3    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 79600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.924    |\n",
            "|    n_updates        | 11305    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-89.97 +/- 57.48\n",
            "Episode length: 943.60 +/- 79.12\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 944      |\n",
            "|    mean_reward      | -90      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 80000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.61     |\n",
            "|    n_updates        | 11330    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80400, episode_reward=-67.62 +/- 29.18\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -67.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 80400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.39     |\n",
            "|    n_updates        | 11355    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80800, episode_reward=-83.56 +/- 26.11\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -83.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 80800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.55     |\n",
            "|    n_updates        | 11380    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=81200, episode_reward=-92.41 +/- 8.34\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -92.4    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 81200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.84     |\n",
            "|    n_updates        | 11405    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=81600, episode_reward=-95.47 +/- 18.93\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -95.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 81600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.41     |\n",
            "|    n_updates        | 11430    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 711      |\n",
            "|    ep_rew_mean      | -29.2    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 156      |\n",
            "|    fps              | 45       |\n",
            "|    time_elapsed     | 1777     |\n",
            "|    total_timesteps  | 81668    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.743    |\n",
            "|    n_updates        | 11435    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=82000, episode_reward=-80.73 +/- 13.04\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -80.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 82000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.816    |\n",
            "|    n_updates        | 11455    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=82400, episode_reward=-76.35 +/- 26.36\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -76.3    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 82400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.764    |\n",
            "|    n_updates        | 11480    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=82800, episode_reward=-87.53 +/- 12.35\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -87.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 82800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.43     |\n",
            "|    n_updates        | 11505    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=83200, episode_reward=-125.85 +/- 51.71\n",
            "Episode length: 989.40 +/- 17.48\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 989      |\n",
            "|    mean_reward      | -126     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 83200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.737    |\n",
            "|    n_updates        | 11530    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=83600, episode_reward=-73.53 +/- 16.33\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -73.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 83600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.1      |\n",
            "|    n_updates        | 11555    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=84000, episode_reward=-102.12 +/- 39.63\n",
            "Episode length: 997.00 +/- 6.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 997      |\n",
            "|    mean_reward      | -102     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 84000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.07     |\n",
            "|    n_updates        | 11580    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=84400, episode_reward=-87.55 +/- 26.15\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -87.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 84400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.27     |\n",
            "|    n_updates        | 11605    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=84800, episode_reward=-105.99 +/- 55.73\n",
            "Episode length: 973.40 +/- 53.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 973      |\n",
            "|    mean_reward      | -106     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 84800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.21     |\n",
            "|    n_updates        | 11630    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=85200, episode_reward=-117.05 +/- 45.00\n",
            "Episode length: 899.80 +/- 127.47\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 900      |\n",
            "|    mean_reward      | -117     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 85200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1        |\n",
            "|    n_updates        | 11655    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=85600, episode_reward=-96.59 +/- 18.15\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -96.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 85600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.674    |\n",
            "|    n_updates        | 11680    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 726      |\n",
            "|    ep_rew_mean      | -30.4    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 160      |\n",
            "|    fps              | 44       |\n",
            "|    time_elapsed     | 1917     |\n",
            "|    total_timesteps  | 85668    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.86     |\n",
            "|    n_updates        | 11685    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=86000, episode_reward=-79.25 +/- 18.75\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -79.3    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 86000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.18     |\n",
            "|    n_updates        | 11705    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=86400, episode_reward=-113.96 +/- 24.64\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -114     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 86400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1        |\n",
            "|    n_updates        | 11730    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=86800, episode_reward=-99.84 +/- 25.80\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -99.8    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 86800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.735    |\n",
            "|    n_updates        | 11755    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=87200, episode_reward=-102.77 +/- 30.72\n",
            "Episode length: 952.40 +/- 95.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 952      |\n",
            "|    mean_reward      | -103     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 87200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.802    |\n",
            "|    n_updates        | 11780    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=87600, episode_reward=-84.30 +/- 29.85\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -84.3    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 87600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.476    |\n",
            "|    n_updates        | 11805    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=88000, episode_reward=-84.03 +/- 18.93\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -84      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 88000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.484    |\n",
            "|    n_updates        | 11830    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=88400, episode_reward=-67.85 +/- 34.11\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -67.9    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 88400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.787    |\n",
            "|    n_updates        | 11855    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=88800, episode_reward=-109.65 +/- 60.49\n",
            "Episode length: 992.00 +/- 16.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 992      |\n",
            "|    mean_reward      | -110     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 88800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.904    |\n",
            "|    n_updates        | 11880    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=89200, episode_reward=-129.51 +/- 40.17\n",
            "Episode length: 855.60 +/- 141.82\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 856      |\n",
            "|    mean_reward      | -130     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 89200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.946    |\n",
            "|    n_updates        | 11905    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=89600, episode_reward=-84.33 +/- 66.33\n",
            "Episode length: 977.40 +/- 45.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 977      |\n",
            "|    mean_reward      | -84.3    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 89600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.12     |\n",
            "|    n_updates        | 11930    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 753      |\n",
            "|    ep_rew_mean      | -35      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 164      |\n",
            "|    fps              | 43       |\n",
            "|    time_elapsed     | 2044     |\n",
            "|    total_timesteps  | 89668    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.07     |\n",
            "|    n_updates        | 11935    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=-62.11 +/- 28.83\n",
            "Episode length: 960.40 +/- 79.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 960      |\n",
            "|    mean_reward      | -62.1    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 90000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.45     |\n",
            "|    n_updates        | 11955    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=90400, episode_reward=-53.36 +/- 27.87\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -53.4    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 90400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.59     |\n",
            "|    n_updates        | 11980    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=90800, episode_reward=-104.25 +/- 49.19\n",
            "Episode length: 996.60 +/- 6.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 997      |\n",
            "|    mean_reward      | -104     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 90800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.1      |\n",
            "|    n_updates        | 12005    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=91200, episode_reward=-81.76 +/- 37.84\n",
            "Episode length: 985.80 +/- 28.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 986      |\n",
            "|    mean_reward      | -81.8    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 91200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.932    |\n",
            "|    n_updates        | 12030    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=91600, episode_reward=-64.58 +/- 14.39\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -64.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 91600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.22     |\n",
            "|    n_updates        | 12055    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=92000, episode_reward=-82.17 +/- 15.01\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -82.2    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 92000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.84     |\n",
            "|    n_updates        | 12080    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=92400, episode_reward=-84.89 +/- 20.21\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -84.9    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 92400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.946    |\n",
            "|    n_updates        | 12105    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=92800, episode_reward=-85.78 +/- 22.83\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -85.8    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 92800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.82     |\n",
            "|    n_updates        | 12130    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=93200, episode_reward=-83.90 +/- 18.45\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -83.9    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 93200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.33     |\n",
            "|    n_updates        | 12155    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=93600, episode_reward=-89.70 +/- 8.50\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -89.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 93600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.798    |\n",
            "|    n_updates        | 12180    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 777      |\n",
            "|    ep_rew_mean      | -37.5    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 168      |\n",
            "|    fps              | 42       |\n",
            "|    time_elapsed     | 2182     |\n",
            "|    total_timesteps  | 93668    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.02     |\n",
            "|    n_updates        | 12185    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=94000, episode_reward=-102.62 +/- 26.39\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -103     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 94000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.854    |\n",
            "|    n_updates        | 12205    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=94400, episode_reward=-99.63 +/- 40.58\n",
            "Episode length: 921.20 +/- 157.60\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 921      |\n",
            "|    mean_reward      | -99.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 94400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.881    |\n",
            "|    n_updates        | 12230    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=94800, episode_reward=-95.29 +/- 28.83\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -95.3    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 94800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.11     |\n",
            "|    n_updates        | 12255    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=95200, episode_reward=-65.08 +/- 10.54\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -65.1    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 95200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.37     |\n",
            "|    n_updates        | 12280    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=95600, episode_reward=-74.74 +/- 13.18\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -74.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 95600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.956    |\n",
            "|    n_updates        | 12305    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=96000, episode_reward=-81.67 +/- 23.96\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -81.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 96000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.14     |\n",
            "|    n_updates        | 12330    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=96400, episode_reward=-79.47 +/- 20.84\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -79.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 96400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.83     |\n",
            "|    n_updates        | 12355    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=96800, episode_reward=-79.49 +/- 19.29\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -79.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 96800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.01     |\n",
            "|    n_updates        | 12380    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=97200, episode_reward=-81.94 +/- 38.29\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -81.9    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 97200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.56     |\n",
            "|    n_updates        | 12405    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=97600, episode_reward=-84.68 +/- 17.56\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -84.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 97600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.91     |\n",
            "|    n_updates        | 12430    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 800      |\n",
            "|    ep_rew_mean      | -43      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 172      |\n",
            "|    fps              | 42       |\n",
            "|    time_elapsed     | 2306     |\n",
            "|    total_timesteps  | 97668    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.18     |\n",
            "|    n_updates        | 12435    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=98000, episode_reward=-92.94 +/- 19.37\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -92.9    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 98000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.639    |\n",
            "|    n_updates        | 12455    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=98400, episode_reward=-78.61 +/- 7.70\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -78.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 98400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.518    |\n",
            "|    n_updates        | 12480    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=98800, episode_reward=-100.43 +/- 23.77\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -100     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 98800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.2      |\n",
            "|    n_updates        | 12505    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=99200, episode_reward=-94.82 +/- 35.91\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -94.8    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 99200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.02     |\n",
            "|    n_updates        | 12530    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=99600, episode_reward=-80.45 +/- 10.90\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -80.4    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 99600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.24     |\n",
            "|    n_updates        | 12555    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=-75.26 +/- 14.72\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -75.3    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 100000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.86     |\n",
            "|    n_updates        | 12580    |\n",
            "----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the PPO agent\n",
        "engine.train_agent(agent=ppo_agent,\n",
        "                   total_timesteps=total_timesteps,\n",
        "                   callback=callback,\n",
        "                   tb_log_name=tb_log_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8sIUbD142SZ",
        "outputId": "9d28b3ec-938d-4b2b-a6fb-263cde5b0ff8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to logs_tensorboard/runs_4\n",
            "Eval num_timesteps=400, episode_reward=-248.22 +/- 166.99\n",
            "Episode length: 64.20 +/- 11.57\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 64.2     |\n",
            "|    mean_reward     | -248     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 400      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=800, episode_reward=-190.70 +/- 130.87\n",
            "Episode length: 62.80 +/- 10.46\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 62.8     |\n",
            "|    mean_reward     | -191     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 800      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1200, episode_reward=-391.21 +/- 223.19\n",
            "Episode length: 66.40 +/- 9.56\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 66.4     |\n",
            "|    mean_reward     | -391     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1200     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1600, episode_reward=-270.09 +/- 177.91\n",
            "Episode length: 65.00 +/- 5.76\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 65       |\n",
            "|    mean_reward     | -270     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1600     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2000, episode_reward=-201.21 +/- 226.06\n",
            "Episode length: 73.00 +/- 10.45\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 73       |\n",
            "|    mean_reward     | -201     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2000     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2400, episode_reward=-369.06 +/- 200.13\n",
            "Episode length: 72.40 +/- 10.37\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 72.4     |\n",
            "|    mean_reward     | -369     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2400     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2800, episode_reward=-241.64 +/- 169.77\n",
            "Episode length: 58.60 +/- 2.24\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 58.6     |\n",
            "|    mean_reward     | -242     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2800     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3200, episode_reward=-193.74 +/- 105.23\n",
            "Episode length: 75.80 +/- 10.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 75.8     |\n",
            "|    mean_reward     | -194     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3200     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3600, episode_reward=-206.12 +/- 152.63\n",
            "Episode length: 67.00 +/- 7.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 67       |\n",
            "|    mean_reward     | -206     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3600     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4000, episode_reward=-425.00 +/- 246.31\n",
            "Episode length: 81.20 +/- 22.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 81.2     |\n",
            "|    mean_reward     | -425     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4000     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4400, episode_reward=-282.52 +/- 202.72\n",
            "Episode length: 75.20 +/- 6.18\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 75.2     |\n",
            "|    mean_reward     | -283     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4400     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4800, episode_reward=-175.97 +/- 104.63\n",
            "Episode length: 61.00 +/- 7.13\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 61       |\n",
            "|    mean_reward     | -176     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4800     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5200, episode_reward=-136.25 +/- 7.30\n",
            "Episode length: 61.20 +/- 4.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 61.2     |\n",
            "|    mean_reward     | -136     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5200     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5600, episode_reward=-364.50 +/- 133.71\n",
            "Episode length: 60.00 +/- 9.25\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 60       |\n",
            "|    mean_reward     | -364     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5600     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=6000, episode_reward=-349.05 +/- 210.06\n",
            "Episode length: 66.40 +/- 10.65\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 66.4     |\n",
            "|    mean_reward     | -349     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 6000     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=6400, episode_reward=-203.05 +/- 147.81\n",
            "Episode length: 69.40 +/- 8.19\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 69.4     |\n",
            "|    mean_reward     | -203     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 6400     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=6800, episode_reward=-215.38 +/- 155.30\n",
            "Episode length: 65.80 +/- 4.02\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 65.8     |\n",
            "|    mean_reward     | -215     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 6800     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=7200, episode_reward=-236.75 +/- 131.93\n",
            "Episode length: 69.20 +/- 13.64\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 69.2     |\n",
            "|    mean_reward     | -237     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 7200     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=7600, episode_reward=-225.66 +/- 219.74\n",
            "Episode length: 64.40 +/- 12.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 64.4     |\n",
            "|    mean_reward     | -226     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 7600     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=8000, episode_reward=-285.86 +/- 178.81\n",
            "Episode length: 65.40 +/- 2.42\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 65.4     |\n",
            "|    mean_reward     | -286     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8000     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 89.8     |\n",
            "|    ep_rew_mean     | -187     |\n",
            "| time/              |          |\n",
            "|    fps             | 904      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 8192     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=8400, episode_reward=-278.29 +/- 238.66\n",
            "Episode length: 774.00 +/- 282.58\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 774         |\n",
            "|    mean_reward          | -278        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 8400        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008890146 |\n",
            "|    clip_fraction        | 0.0403      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.38       |\n",
            "|    explained_variance   | 0.00494     |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 349         |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00801    |\n",
            "|    value_loss           | 1.57e+03    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=8800, episode_reward=-258.94 +/- 227.30\n",
            "Episode length: 464.80 +/- 437.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 465      |\n",
            "|    mean_reward     | -259     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8800     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=9200, episode_reward=-407.93 +/- 280.05\n",
            "Episode length: 522.40 +/- 394.26\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 522      |\n",
            "|    mean_reward     | -408     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 9200     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=9600, episode_reward=-180.38 +/- 219.99\n",
            "Episode length: 868.60 +/- 262.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 869      |\n",
            "|    mean_reward     | -180     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 9600     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=-540.81 +/- 245.06\n",
            "Episode length: 364.60 +/- 354.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 365      |\n",
            "|    mean_reward     | -541     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 10000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=10400, episode_reward=-84.02 +/- 12.45\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | -84      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 10400    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=10800, episode_reward=-406.29 +/- 248.13\n",
            "Episode length: 518.20 +/- 396.59\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 518      |\n",
            "|    mean_reward     | -406     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 10800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=11200, episode_reward=-142.77 +/- 136.03\n",
            "Episode length: 845.40 +/- 309.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 845      |\n",
            "|    mean_reward     | -143     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 11200    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=11600, episode_reward=-168.24 +/- 158.90\n",
            "Episode length: 822.80 +/- 354.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 823      |\n",
            "|    mean_reward     | -168     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 11600    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=12000, episode_reward=-266.23 +/- 236.88\n",
            "Episode length: 694.40 +/- 374.73\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 694      |\n",
            "|    mean_reward     | -266     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 12000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=12400, episode_reward=-386.61 +/- 243.62\n",
            "Episode length: 507.60 +/- 404.18\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 508      |\n",
            "|    mean_reward     | -387     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 12400    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=12800, episode_reward=-79.07 +/- 5.63\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | -79.1    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 12800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=13200, episode_reward=-307.48 +/- 283.33\n",
            "Episode length: 659.20 +/- 418.59\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 659      |\n",
            "|    mean_reward     | -307     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 13200    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=13600, episode_reward=-221.97 +/- 187.11\n",
            "Episode length: 664.40 +/- 412.01\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 664      |\n",
            "|    mean_reward     | -222     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 13600    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=14000, episode_reward=-218.95 +/- 196.62\n",
            "Episode length: 649.40 +/- 429.42\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 649      |\n",
            "|    mean_reward     | -219     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 14000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=14400, episode_reward=-176.85 +/- 194.16\n",
            "Episode length: 853.20 +/- 293.60\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 853      |\n",
            "|    mean_reward     | -177     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 14400    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=14800, episode_reward=-242.51 +/- 192.93\n",
            "Episode length: 639.20 +/- 441.99\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 639      |\n",
            "|    mean_reward     | -243     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 14800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=15200, episode_reward=-247.01 +/- 223.31\n",
            "Episode length: 702.60 +/- 373.06\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 703      |\n",
            "|    mean_reward     | -247     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 15200    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=15600, episode_reward=-290.97 +/- 269.12\n",
            "Episode length: 693.80 +/- 384.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 694      |\n",
            "|    mean_reward     | -291     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 15600    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=-205.46 +/- 139.21\n",
            "Episode length: 685.80 +/- 384.87\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 686      |\n",
            "|    mean_reward     | -205     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 16000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 95.5     |\n",
            "|    ep_rew_mean     | -161     |\n",
            "| time/              |          |\n",
            "|    fps             | 76       |\n",
            "|    iterations      | 2        |\n",
            "|    time_elapsed    | 214      |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=16400, episode_reward=-465.11 +/- 45.40\n",
            "Episode length: 209.00 +/- 19.94\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 209         |\n",
            "|    mean_reward          | -465        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 16400       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008114187 |\n",
            "|    clip_fraction        | 0.0247      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.36       |\n",
            "|    explained_variance   | -0.00756    |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 336         |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00647    |\n",
            "|    value_loss           | 983         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=16800, episode_reward=-507.79 +/- 45.27\n",
            "Episode length: 238.40 +/- 35.85\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 238      |\n",
            "|    mean_reward     | -508     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 16800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=17200, episode_reward=-552.34 +/- 155.31\n",
            "Episode length: 257.20 +/- 89.54\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 257      |\n",
            "|    mean_reward     | -552     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 17200    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=17600, episode_reward=-639.35 +/- 179.46\n",
            "Episode length: 288.60 +/- 97.90\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 289      |\n",
            "|    mean_reward     | -639     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 17600    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=18000, episode_reward=-533.22 +/- 25.29\n",
            "Episode length: 237.80 +/- 27.64\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 238      |\n",
            "|    mean_reward     | -533     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 18000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=18400, episode_reward=-471.51 +/- 60.80\n",
            "Episode length: 254.00 +/- 34.57\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 254      |\n",
            "|    mean_reward     | -472     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 18400    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=18800, episode_reward=-545.10 +/- 163.81\n",
            "Episode length: 258.60 +/- 82.58\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 259      |\n",
            "|    mean_reward     | -545     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 18800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=19200, episode_reward=-461.68 +/- 80.13\n",
            "Episode length: 236.60 +/- 31.19\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 237      |\n",
            "|    mean_reward     | -462     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 19200    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=19600, episode_reward=-477.35 +/- 40.22\n",
            "Episode length: 244.40 +/- 25.06\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 244      |\n",
            "|    mean_reward     | -477     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 19600    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=-498.35 +/- 128.72\n",
            "Episode length: 287.40 +/- 93.76\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 287      |\n",
            "|    mean_reward     | -498     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 20000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=20400, episode_reward=-449.95 +/- 74.20\n",
            "Episode length: 226.00 +/- 34.15\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 226      |\n",
            "|    mean_reward     | -450     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 20400    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=20800, episode_reward=-564.65 +/- 146.52\n",
            "Episode length: 259.80 +/- 69.73\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 260      |\n",
            "|    mean_reward     | -565     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 20800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=21200, episode_reward=-503.14 +/- 52.67\n",
            "Episode length: 237.20 +/- 49.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 237      |\n",
            "|    mean_reward     | -503     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 21200    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=21600, episode_reward=-534.37 +/- 104.95\n",
            "Episode length: 232.20 +/- 43.76\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 232      |\n",
            "|    mean_reward     | -534     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 21600    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=22000, episode_reward=-509.19 +/- 112.57\n",
            "Episode length: 253.00 +/- 69.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 253      |\n",
            "|    mean_reward     | -509     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 22000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=22400, episode_reward=-561.83 +/- 207.52\n",
            "Episode length: 287.00 +/- 82.98\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 287      |\n",
            "|    mean_reward     | -562     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 22400    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=22800, episode_reward=-491.36 +/- 82.47\n",
            "Episode length: 253.80 +/- 50.03\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 254      |\n",
            "|    mean_reward     | -491     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 22800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=23200, episode_reward=-437.15 +/- 33.65\n",
            "Episode length: 232.20 +/- 17.81\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 232      |\n",
            "|    mean_reward     | -437     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 23200    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=23600, episode_reward=-547.60 +/- 237.19\n",
            "Episode length: 257.60 +/- 93.56\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 258      |\n",
            "|    mean_reward     | -548     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 23600    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=-468.81 +/- 62.03\n",
            "Episode length: 219.00 +/- 24.73\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 219      |\n",
            "|    mean_reward     | -469     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 24000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=24400, episode_reward=-525.03 +/- 97.23\n",
            "Episode length: 300.00 +/- 84.14\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 300      |\n",
            "|    mean_reward     | -525     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 24400    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 103      |\n",
            "|    ep_rew_mean     | -147     |\n",
            "| time/              |          |\n",
            "|    fps             | 98       |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 250      |\n",
            "|    total_timesteps | 24576    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=24800, episode_reward=-3264.85 +/- 122.95\n",
            "Episode length: 910.00 +/- 14.82\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 910         |\n",
            "|    mean_reward          | -3.26e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 24800       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004749673 |\n",
            "|    clip_fraction        | 0.0215      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.35       |\n",
            "|    explained_variance   | -0.0206     |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 418         |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00537    |\n",
            "|    value_loss           | 641         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=25200, episode_reward=-3214.85 +/- 160.40\n",
            "Episode length: 945.00 +/- 41.43\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 945       |\n",
            "|    mean_reward     | -3.21e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 25200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=25600, episode_reward=-3325.17 +/- 231.88\n",
            "Episode length: 930.60 +/- 32.51\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 931       |\n",
            "|    mean_reward     | -3.33e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 25600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=26000, episode_reward=-3206.18 +/- 143.50\n",
            "Episode length: 923.00 +/- 27.33\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 923       |\n",
            "|    mean_reward     | -3.21e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 26000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=26400, episode_reward=-3313.68 +/- 47.98\n",
            "Episode length: 911.00 +/- 7.54\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 911       |\n",
            "|    mean_reward     | -3.31e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 26400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=26800, episode_reward=-2819.46 +/- 593.47\n",
            "Episode length: 974.20 +/- 31.79\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 974       |\n",
            "|    mean_reward     | -2.82e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 26800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=27200, episode_reward=-3175.62 +/- 127.67\n",
            "Episode length: 912.40 +/- 17.44\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 912       |\n",
            "|    mean_reward     | -3.18e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 27200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=27600, episode_reward=-3139.57 +/- 236.78\n",
            "Episode length: 906.40 +/- 12.50\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 906       |\n",
            "|    mean_reward     | -3.14e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 27600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=28000, episode_reward=-2692.86 +/- 1182.80\n",
            "Episode length: 823.20 +/- 187.33\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 823       |\n",
            "|    mean_reward     | -2.69e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 28000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=28400, episode_reward=-2943.89 +/- 580.38\n",
            "Episode length: 937.80 +/- 35.99\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 938       |\n",
            "|    mean_reward     | -2.94e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 28400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=28800, episode_reward=-3313.60 +/- 146.52\n",
            "Episode length: 951.20 +/- 42.07\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 951       |\n",
            "|    mean_reward     | -3.31e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 28800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=29200, episode_reward=-3226.77 +/- 235.99\n",
            "Episode length: 933.40 +/- 34.67\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 933       |\n",
            "|    mean_reward     | -3.23e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 29200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=29600, episode_reward=-3237.93 +/- 90.43\n",
            "Episode length: 928.60 +/- 37.62\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 929       |\n",
            "|    mean_reward     | -3.24e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 29600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=-3174.61 +/- 182.33\n",
            "Episode length: 918.40 +/- 31.70\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 918       |\n",
            "|    mean_reward     | -3.17e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 30000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30400, episode_reward=-3334.98 +/- 132.12\n",
            "Episode length: 943.40 +/- 38.79\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 943       |\n",
            "|    mean_reward     | -3.33e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 30400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30800, episode_reward=-3266.65 +/- 216.40\n",
            "Episode length: 928.60 +/- 31.13\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 929       |\n",
            "|    mean_reward     | -3.27e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 30800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=31200, episode_reward=-3216.71 +/- 141.23\n",
            "Episode length: 932.20 +/- 34.20\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 932       |\n",
            "|    mean_reward     | -3.22e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 31200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=31600, episode_reward=-2654.32 +/- 1097.00\n",
            "Episode length: 826.20 +/- 187.26\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 826       |\n",
            "|    mean_reward     | -2.65e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 31600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=-3333.58 +/- 91.01\n",
            "Episode length: 947.40 +/- 32.75\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 947       |\n",
            "|    mean_reward     | -3.33e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 32000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=32400, episode_reward=-2906.73 +/- 616.59\n",
            "Episode length: 924.00 +/- 38.43\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 924       |\n",
            "|    mean_reward     | -2.91e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 32400     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 116      |\n",
            "|    ep_rew_mean     | -131     |\n",
            "| time/              |          |\n",
            "|    fps             | 68       |\n",
            "|    iterations      | 4        |\n",
            "|    time_elapsed    | 480      |\n",
            "|    total_timesteps | 32768    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=32800, episode_reward=-1960.62 +/- 231.42\n",
            "Episode length: 334.40 +/- 24.94\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 334         |\n",
            "|    mean_reward          | -1.96e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 32800       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009660188 |\n",
            "|    clip_fraction        | 0.0569      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.32       |\n",
            "|    explained_variance   | -0.00132    |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 203         |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.007      |\n",
            "|    value_loss           | 529         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=33200, episode_reward=-2131.44 +/- 1022.56\n",
            "Episode length: 371.40 +/- 62.48\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 371       |\n",
            "|    mean_reward     | -2.13e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 33200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=33600, episode_reward=-1892.97 +/- 253.05\n",
            "Episode length: 350.80 +/- 37.43\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 351       |\n",
            "|    mean_reward     | -1.89e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 33600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=34000, episode_reward=-2377.84 +/- 920.09\n",
            "Episode length: 377.20 +/- 76.87\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 377       |\n",
            "|    mean_reward     | -2.38e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 34000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=34400, episode_reward=-1854.43 +/- 976.31\n",
            "Episode length: 349.60 +/- 54.44\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 350       |\n",
            "|    mean_reward     | -1.85e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 34400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=34800, episode_reward=-1726.08 +/- 183.76\n",
            "Episode length: 350.40 +/- 40.74\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 350       |\n",
            "|    mean_reward     | -1.73e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 34800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=35200, episode_reward=-2036.42 +/- 127.41\n",
            "Episode length: 354.80 +/- 11.03\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 355       |\n",
            "|    mean_reward     | -2.04e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 35200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=35600, episode_reward=-1665.83 +/- 746.21\n",
            "Episode length: 364.40 +/- 57.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 364       |\n",
            "|    mean_reward     | -1.67e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 35600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=36000, episode_reward=-1707.56 +/- 497.74\n",
            "Episode length: 350.80 +/- 30.39\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 351       |\n",
            "|    mean_reward     | -1.71e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 36000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=36400, episode_reward=-1828.40 +/- 147.33\n",
            "Episode length: 363.20 +/- 41.30\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 363       |\n",
            "|    mean_reward     | -1.83e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 36400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=36800, episode_reward=-1561.72 +/- 241.32\n",
            "Episode length: 344.60 +/- 37.53\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 345       |\n",
            "|    mean_reward     | -1.56e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 36800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=37200, episode_reward=-1426.88 +/- 575.82\n",
            "Episode length: 330.00 +/- 54.91\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 330       |\n",
            "|    mean_reward     | -1.43e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 37200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=37600, episode_reward=-1577.31 +/- 623.53\n",
            "Episode length: 333.60 +/- 26.45\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 334       |\n",
            "|    mean_reward     | -1.58e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 37600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=38000, episode_reward=-1703.14 +/- 281.89\n",
            "Episode length: 336.60 +/- 23.30\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 337      |\n",
            "|    mean_reward     | -1.7e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 38000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=38400, episode_reward=-1537.00 +/- 688.33\n",
            "Episode length: 346.00 +/- 49.63\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 346       |\n",
            "|    mean_reward     | -1.54e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 38400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=38800, episode_reward=-1804.32 +/- 1167.75\n",
            "Episode length: 342.80 +/- 62.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 343      |\n",
            "|    mean_reward     | -1.8e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 38800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=39200, episode_reward=-1690.18 +/- 288.00\n",
            "Episode length: 338.60 +/- 17.78\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 339       |\n",
            "|    mean_reward     | -1.69e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 39200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=39600, episode_reward=-1598.04 +/- 555.21\n",
            "Episode length: 329.80 +/- 41.11\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 330      |\n",
            "|    mean_reward     | -1.6e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 39600    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-1972.39 +/- 297.39\n",
            "Episode length: 343.00 +/- 36.95\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 343       |\n",
            "|    mean_reward     | -1.97e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 40000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=40400, episode_reward=-2392.48 +/- 1268.68\n",
            "Episode length: 376.80 +/- 103.86\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 377       |\n",
            "|    mean_reward     | -2.39e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 40400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=40800, episode_reward=-2057.32 +/- 1130.68\n",
            "Episode length: 373.20 +/- 80.42\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 373       |\n",
            "|    mean_reward     | -2.06e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 40800     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 115      |\n",
            "|    ep_rew_mean     | -126     |\n",
            "| time/              |          |\n",
            "|    fps             | 76       |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 535      |\n",
            "|    total_timesteps | 40960    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=41200, episode_reward=-4848.21 +/- 637.91\n",
            "Episode length: 745.80 +/- 42.19\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 746        |\n",
            "|    mean_reward          | -4.85e+03  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 41200      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00626563 |\n",
            "|    clip_fraction        | 0.0411     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.29      |\n",
            "|    explained_variance   | -0.00885   |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | 203        |\n",
            "|    n_updates            | 50         |\n",
            "|    policy_gradient_loss | -0.00496   |\n",
            "|    value_loss           | 492        |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=41600, episode_reward=-4630.50 +/- 737.39\n",
            "Episode length: 758.40 +/- 37.66\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 758       |\n",
            "|    mean_reward     | -4.63e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 41600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=42000, episode_reward=-4922.38 +/- 856.21\n",
            "Episode length: 757.60 +/- 57.79\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 758       |\n",
            "|    mean_reward     | -4.92e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 42000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=42400, episode_reward=-4698.02 +/- 562.87\n",
            "Episode length: 745.40 +/- 43.04\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 745      |\n",
            "|    mean_reward     | -4.7e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 42400    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=42800, episode_reward=-5988.80 +/- 2572.82\n",
            "Episode length: 795.20 +/- 111.18\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 795       |\n",
            "|    mean_reward     | -5.99e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 42800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=43200, episode_reward=-4435.67 +/- 1049.06\n",
            "Episode length: 740.40 +/- 43.72\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 740       |\n",
            "|    mean_reward     | -4.44e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 43200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=43600, episode_reward=-4157.77 +/- 329.74\n",
            "Episode length: 707.40 +/- 16.35\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 707       |\n",
            "|    mean_reward     | -4.16e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 43600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=44000, episode_reward=-4439.91 +/- 667.72\n",
            "Episode length: 761.00 +/- 73.33\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 761       |\n",
            "|    mean_reward     | -4.44e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 44000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=44400, episode_reward=-6396.60 +/- 1982.59\n",
            "Episode length: 821.00 +/- 84.06\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 821      |\n",
            "|    mean_reward     | -6.4e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 44400    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=44800, episode_reward=-4903.21 +/- 599.81\n",
            "Episode length: 761.20 +/- 49.91\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 761      |\n",
            "|    mean_reward     | -4.9e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 44800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=45200, episode_reward=-4567.35 +/- 555.98\n",
            "Episode length: 738.60 +/- 30.16\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 739       |\n",
            "|    mean_reward     | -4.57e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 45200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=45600, episode_reward=-5842.91 +/- 1293.69\n",
            "Episode length: 817.00 +/- 87.01\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 817       |\n",
            "|    mean_reward     | -5.84e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 45600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=46000, episode_reward=-5679.48 +/- 2403.36\n",
            "Episode length: 790.00 +/- 117.34\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 790       |\n",
            "|    mean_reward     | -5.68e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 46000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=46400, episode_reward=-4540.42 +/- 551.57\n",
            "Episode length: 738.60 +/- 46.86\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 739       |\n",
            "|    mean_reward     | -4.54e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 46400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=46800, episode_reward=-4815.59 +/- 1039.38\n",
            "Episode length: 748.80 +/- 61.84\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 749       |\n",
            "|    mean_reward     | -4.82e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 46800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=47200, episode_reward=-4331.77 +/- 280.36\n",
            "Episode length: 711.60 +/- 15.47\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 712       |\n",
            "|    mean_reward     | -4.33e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 47200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=47600, episode_reward=-5172.51 +/- 1616.59\n",
            "Episode length: 773.80 +/- 87.03\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 774       |\n",
            "|    mean_reward     | -5.17e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 47600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=-4545.10 +/- 777.71\n",
            "Episode length: 771.80 +/- 28.41\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 772       |\n",
            "|    mean_reward     | -4.55e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 48000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=48400, episode_reward=-4476.91 +/- 556.56\n",
            "Episode length: 736.20 +/- 52.04\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 736       |\n",
            "|    mean_reward     | -4.48e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 48400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=48800, episode_reward=-4657.01 +/- 398.11\n",
            "Episode length: 774.00 +/- 57.93\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 774       |\n",
            "|    mean_reward     | -4.66e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 48800     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 113      |\n",
            "|    ep_rew_mean     | -103     |\n",
            "| time/              |          |\n",
            "|    fps             | 70       |\n",
            "|    iterations      | 6        |\n",
            "|    time_elapsed    | 696      |\n",
            "|    total_timesteps | 49152    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=49200, episode_reward=-4257.52 +/- 1591.46\n",
            "Episode length: 713.60 +/- 75.01\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 714         |\n",
            "|    mean_reward          | -4.26e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 49200       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008280546 |\n",
            "|    clip_fraction        | 0.0488      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.26       |\n",
            "|    explained_variance   | -0.00939    |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 147         |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.00635    |\n",
            "|    value_loss           | 348         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=49600, episode_reward=-4533.66 +/- 977.23\n",
            "Episode length: 714.60 +/- 57.42\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 715       |\n",
            "|    mean_reward     | -4.53e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 49600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=-3521.91 +/- 1894.30\n",
            "Episode length: 673.00 +/- 113.50\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 673       |\n",
            "|    mean_reward     | -3.52e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 50000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50400, episode_reward=-4482.17 +/- 816.37\n",
            "Episode length: 792.60 +/- 39.99\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 793       |\n",
            "|    mean_reward     | -4.48e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 50400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50800, episode_reward=-4478.56 +/- 793.79\n",
            "Episode length: 770.60 +/- 21.40\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 771       |\n",
            "|    mean_reward     | -4.48e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 50800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=51200, episode_reward=-3730.09 +/- 567.52\n",
            "Episode length: 702.40 +/- 35.90\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 702       |\n",
            "|    mean_reward     | -3.73e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 51200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=51600, episode_reward=-4035.26 +/- 734.90\n",
            "Episode length: 715.80 +/- 36.94\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 716       |\n",
            "|    mean_reward     | -4.04e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 51600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=52000, episode_reward=-3983.64 +/- 596.82\n",
            "Episode length: 712.80 +/- 53.12\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 713       |\n",
            "|    mean_reward     | -3.98e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 52000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=52400, episode_reward=-4504.84 +/- 1625.11\n",
            "Episode length: 784.00 +/- 74.74\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 784      |\n",
            "|    mean_reward     | -4.5e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 52400    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=52800, episode_reward=-4707.24 +/- 1038.72\n",
            "Episode length: 733.40 +/- 78.58\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 733       |\n",
            "|    mean_reward     | -4.71e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 52800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=53200, episode_reward=-4293.17 +/- 715.06\n",
            "Episode length: 734.20 +/- 53.30\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 734       |\n",
            "|    mean_reward     | -4.29e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 53200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=53600, episode_reward=-4281.55 +/- 1036.01\n",
            "Episode length: 742.60 +/- 106.58\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 743       |\n",
            "|    mean_reward     | -4.28e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 53600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=54000, episode_reward=-4135.58 +/- 766.99\n",
            "Episode length: 693.00 +/- 41.76\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 693       |\n",
            "|    mean_reward     | -4.14e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 54000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=54400, episode_reward=-4146.88 +/- 1270.10\n",
            "Episode length: 700.80 +/- 60.87\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 701       |\n",
            "|    mean_reward     | -4.15e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 54400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=54800, episode_reward=-4674.05 +/- 787.12\n",
            "Episode length: 752.20 +/- 42.56\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 752       |\n",
            "|    mean_reward     | -4.67e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 54800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=55200, episode_reward=-3767.09 +/- 731.26\n",
            "Episode length: 703.20 +/- 64.98\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 703       |\n",
            "|    mean_reward     | -3.77e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 55200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=55600, episode_reward=-4531.69 +/- 896.56\n",
            "Episode length: 747.00 +/- 51.66\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 747       |\n",
            "|    mean_reward     | -4.53e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 55600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=56000, episode_reward=-3842.70 +/- 676.93\n",
            "Episode length: 683.00 +/- 41.05\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 683       |\n",
            "|    mean_reward     | -3.84e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 56000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=56400, episode_reward=-3515.03 +/- 279.71\n",
            "Episode length: 672.60 +/- 27.55\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 673       |\n",
            "|    mean_reward     | -3.52e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 56400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=56800, episode_reward=-4096.78 +/- 918.93\n",
            "Episode length: 694.80 +/- 51.34\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 695      |\n",
            "|    mean_reward     | -4.1e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 56800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=57200, episode_reward=-3545.31 +/- 608.20\n",
            "Episode length: 701.80 +/- 52.43\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 702       |\n",
            "|    mean_reward     | -3.55e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 57200     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 129      |\n",
            "|    ep_rew_mean     | -99.9    |\n",
            "| time/              |          |\n",
            "|    fps             | 67       |\n",
            "|    iterations      | 7        |\n",
            "|    time_elapsed    | 848      |\n",
            "|    total_timesteps | 57344    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=57600, episode_reward=-1451.35 +/- 189.99\n",
            "Episode length: 401.20 +/- 13.89\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 401         |\n",
            "|    mean_reward          | -1.45e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 57600       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010022926 |\n",
            "|    clip_fraction        | 0.0409      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | -0.0642     |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 198         |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.00602    |\n",
            "|    value_loss           | 371         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=58000, episode_reward=-1631.56 +/- 268.57\n",
            "Episode length: 429.00 +/- 37.73\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 429       |\n",
            "|    mean_reward     | -1.63e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 58000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=58400, episode_reward=-1448.76 +/- 278.55\n",
            "Episode length: 414.60 +/- 44.90\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 415       |\n",
            "|    mean_reward     | -1.45e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 58400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=58800, episode_reward=-1825.79 +/- 146.12\n",
            "Episode length: 461.80 +/- 49.60\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 462       |\n",
            "|    mean_reward     | -1.83e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 58800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=59200, episode_reward=-1661.28 +/- 229.67\n",
            "Episode length: 438.40 +/- 47.22\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 438       |\n",
            "|    mean_reward     | -1.66e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 59200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=59600, episode_reward=-1450.06 +/- 306.77\n",
            "Episode length: 401.60 +/- 57.86\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 402       |\n",
            "|    mean_reward     | -1.45e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 59600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=-1738.39 +/- 224.15\n",
            "Episode length: 447.80 +/- 26.35\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 448       |\n",
            "|    mean_reward     | -1.74e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 60000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60400, episode_reward=-1518.42 +/- 200.28\n",
            "Episode length: 402.40 +/- 12.89\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 402       |\n",
            "|    mean_reward     | -1.52e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 60400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60800, episode_reward=-1675.14 +/- 155.14\n",
            "Episode length: 418.80 +/- 32.47\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 419       |\n",
            "|    mean_reward     | -1.68e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 60800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=61200, episode_reward=-1484.41 +/- 150.26\n",
            "Episode length: 404.20 +/- 15.38\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 404       |\n",
            "|    mean_reward     | -1.48e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 61200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=61600, episode_reward=-1560.11 +/- 206.02\n",
            "Episode length: 406.80 +/- 28.84\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 407       |\n",
            "|    mean_reward     | -1.56e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 61600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=62000, episode_reward=-1627.64 +/- 276.83\n",
            "Episode length: 434.40 +/- 48.92\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 434       |\n",
            "|    mean_reward     | -1.63e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 62000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=62400, episode_reward=-1643.91 +/- 196.53\n",
            "Episode length: 417.20 +/- 44.94\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 417       |\n",
            "|    mean_reward     | -1.64e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 62400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=62800, episode_reward=-1699.18 +/- 217.33\n",
            "Episode length: 428.20 +/- 51.76\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 428      |\n",
            "|    mean_reward     | -1.7e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 62800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=63200, episode_reward=-1563.64 +/- 202.04\n",
            "Episode length: 418.20 +/- 48.16\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 418       |\n",
            "|    mean_reward     | -1.56e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 63200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=63600, episode_reward=-1507.66 +/- 297.08\n",
            "Episode length: 416.40 +/- 37.15\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 416       |\n",
            "|    mean_reward     | -1.51e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 63600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=64000, episode_reward=-1639.26 +/- 200.84\n",
            "Episode length: 427.40 +/- 39.78\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 427       |\n",
            "|    mean_reward     | -1.64e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 64000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=64400, episode_reward=-1325.35 +/- 213.20\n",
            "Episode length: 395.80 +/- 13.56\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 396       |\n",
            "|    mean_reward     | -1.33e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 64400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=64800, episode_reward=-1433.99 +/- 349.05\n",
            "Episode length: 400.00 +/- 28.40\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 400       |\n",
            "|    mean_reward     | -1.43e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 64800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=65200, episode_reward=-1409.91 +/- 359.71\n",
            "Episode length: 416.80 +/- 26.23\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 417       |\n",
            "|    mean_reward     | -1.41e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 65200     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 149      |\n",
            "|    ep_rew_mean     | -90.9    |\n",
            "| time/              |          |\n",
            "|    fps             | 71       |\n",
            "|    iterations      | 8        |\n",
            "|    time_elapsed    | 913      |\n",
            "|    total_timesteps | 65536    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=65600, episode_reward=-1409.58 +/- 403.86\n",
            "Episode length: 418.20 +/- 65.86\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 418         |\n",
            "|    mean_reward          | -1.41e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 65600       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011920803 |\n",
            "|    clip_fraction        | 0.0538      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.15       |\n",
            "|    explained_variance   | -0.0326     |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 238         |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.00679    |\n",
            "|    value_loss           | 391         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=66000, episode_reward=-1514.17 +/- 165.99\n",
            "Episode length: 400.80 +/- 32.28\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 401       |\n",
            "|    mean_reward     | -1.51e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 66000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=66400, episode_reward=-1483.80 +/- 260.13\n",
            "Episode length: 406.40 +/- 52.22\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 406       |\n",
            "|    mean_reward     | -1.48e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 66400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=66800, episode_reward=-1627.89 +/- 182.64\n",
            "Episode length: 436.60 +/- 57.18\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 437       |\n",
            "|    mean_reward     | -1.63e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 66800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=67200, episode_reward=-1416.16 +/- 123.02\n",
            "Episode length: 389.40 +/- 15.42\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 389       |\n",
            "|    mean_reward     | -1.42e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 67200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=67600, episode_reward=-1434.16 +/- 225.87\n",
            "Episode length: 391.60 +/- 26.26\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 392       |\n",
            "|    mean_reward     | -1.43e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 67600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=68000, episode_reward=-1328.40 +/- 260.42\n",
            "Episode length: 394.80 +/- 37.16\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 395       |\n",
            "|    mean_reward     | -1.33e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 68000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=68400, episode_reward=-1647.93 +/- 42.52\n",
            "Episode length: 419.60 +/- 14.75\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 420       |\n",
            "|    mean_reward     | -1.65e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 68400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=68800, episode_reward=-1339.80 +/- 250.23\n",
            "Episode length: 391.40 +/- 33.18\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 391       |\n",
            "|    mean_reward     | -1.34e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 68800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=69200, episode_reward=-1617.10 +/- 324.81\n",
            "Episode length: 450.60 +/- 62.14\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 451       |\n",
            "|    mean_reward     | -1.62e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 69200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=69600, episode_reward=-1514.01 +/- 353.70\n",
            "Episode length: 426.40 +/- 39.86\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 426       |\n",
            "|    mean_reward     | -1.51e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 69600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=-1242.13 +/- 256.20\n",
            "Episode length: 376.20 +/- 16.15\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 376       |\n",
            "|    mean_reward     | -1.24e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 70000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70400, episode_reward=-1484.20 +/- 252.47\n",
            "Episode length: 406.60 +/- 39.58\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 407       |\n",
            "|    mean_reward     | -1.48e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 70400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70800, episode_reward=-1379.75 +/- 204.11\n",
            "Episode length: 397.60 +/- 22.91\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 398       |\n",
            "|    mean_reward     | -1.38e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 70800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=71200, episode_reward=-1299.61 +/- 284.12\n",
            "Episode length: 393.40 +/- 27.88\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 393      |\n",
            "|    mean_reward     | -1.3e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 71200    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=71600, episode_reward=-1663.04 +/- 159.35\n",
            "Episode length: 434.80 +/- 45.97\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 435       |\n",
            "|    mean_reward     | -1.66e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 71600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=72000, episode_reward=-1339.08 +/- 300.75\n",
            "Episode length: 387.00 +/- 19.47\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 387       |\n",
            "|    mean_reward     | -1.34e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 72000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=72400, episode_reward=-1638.04 +/- 142.65\n",
            "Episode length: 430.40 +/- 37.23\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 430       |\n",
            "|    mean_reward     | -1.64e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 72400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=72800, episode_reward=-1550.12 +/- 114.52\n",
            "Episode length: 409.40 +/- 9.39\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 409       |\n",
            "|    mean_reward     | -1.55e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 72800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=73200, episode_reward=-1460.21 +/- 182.29\n",
            "Episode length: 396.80 +/- 39.27\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 397       |\n",
            "|    mean_reward     | -1.46e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 73200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=73600, episode_reward=-1275.24 +/- 242.84\n",
            "Episode length: 380.80 +/- 24.99\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 381       |\n",
            "|    mean_reward     | -1.28e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 73600     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 178      |\n",
            "|    ep_rew_mean     | -76.5    |\n",
            "| time/              |          |\n",
            "|    fps             | 75       |\n",
            "|    iterations      | 9        |\n",
            "|    time_elapsed    | 982      |\n",
            "|    total_timesteps | 73728    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=74000, episode_reward=-1588.76 +/- 196.05\n",
            "Episode length: 422.20 +/- 51.27\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 422         |\n",
            "|    mean_reward          | -1.59e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 74000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007009428 |\n",
            "|    clip_fraction        | 0.0106      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.17       |\n",
            "|    explained_variance   | 0.0571      |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 77.1        |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.00239    |\n",
            "|    value_loss           | 231         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=74400, episode_reward=-1532.27 +/- 182.72\n",
            "Episode length: 411.20 +/- 45.33\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 411       |\n",
            "|    mean_reward     | -1.53e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 74400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=74800, episode_reward=-1433.06 +/- 295.88\n",
            "Episode length: 405.60 +/- 59.26\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 406       |\n",
            "|    mean_reward     | -1.43e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 74800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=75200, episode_reward=-1380.14 +/- 361.76\n",
            "Episode length: 391.60 +/- 60.04\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 392       |\n",
            "|    mean_reward     | -1.38e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 75200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=75600, episode_reward=-1522.86 +/- 110.49\n",
            "Episode length: 416.60 +/- 23.14\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 417       |\n",
            "|    mean_reward     | -1.52e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 75600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=76000, episode_reward=-1236.14 +/- 285.46\n",
            "Episode length: 379.40 +/- 50.74\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 379       |\n",
            "|    mean_reward     | -1.24e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 76000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=76400, episode_reward=-1527.13 +/- 142.82\n",
            "Episode length: 405.60 +/- 27.59\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 406       |\n",
            "|    mean_reward     | -1.53e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 76400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=76800, episode_reward=-1453.25 +/- 224.56\n",
            "Episode length: 402.40 +/- 39.47\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 402       |\n",
            "|    mean_reward     | -1.45e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 76800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=77200, episode_reward=-1483.59 +/- 234.57\n",
            "Episode length: 416.20 +/- 58.83\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 416       |\n",
            "|    mean_reward     | -1.48e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 77200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=77600, episode_reward=-1233.26 +/- 264.05\n",
            "Episode length: 363.60 +/- 22.86\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 364       |\n",
            "|    mean_reward     | -1.23e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 77600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=78000, episode_reward=-1426.37 +/- 265.19\n",
            "Episode length: 393.20 +/- 47.13\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 393       |\n",
            "|    mean_reward     | -1.43e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 78000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=78400, episode_reward=-1129.40 +/- 259.47\n",
            "Episode length: 356.80 +/- 28.91\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 357       |\n",
            "|    mean_reward     | -1.13e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 78400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=78800, episode_reward=-1540.40 +/- 245.79\n",
            "Episode length: 421.00 +/- 52.44\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 421       |\n",
            "|    mean_reward     | -1.54e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 78800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=79200, episode_reward=-1461.88 +/- 221.58\n",
            "Episode length: 403.80 +/- 49.61\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 404       |\n",
            "|    mean_reward     | -1.46e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 79200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=79600, episode_reward=-1241.62 +/- 287.51\n",
            "Episode length: 383.40 +/- 56.82\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 383       |\n",
            "|    mean_reward     | -1.24e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 79600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-1389.98 +/- 181.58\n",
            "Episode length: 383.00 +/- 28.78\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 383       |\n",
            "|    mean_reward     | -1.39e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 80000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80400, episode_reward=-1361.74 +/- 197.42\n",
            "Episode length: 392.80 +/- 27.67\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 393       |\n",
            "|    mean_reward     | -1.36e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 80400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80800, episode_reward=-1300.71 +/- 214.99\n",
            "Episode length: 380.00 +/- 19.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 380      |\n",
            "|    mean_reward     | -1.3e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 80800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=81200, episode_reward=-1231.82 +/- 295.99\n",
            "Episode length: 364.20 +/- 41.54\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 364       |\n",
            "|    mean_reward     | -1.23e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 81200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=81600, episode_reward=-1248.09 +/- 102.70\n",
            "Episode length: 357.40 +/- 8.87\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 357       |\n",
            "|    mean_reward     | -1.25e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 81600     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 225      |\n",
            "|    ep_rew_mean     | -92      |\n",
            "| time/              |          |\n",
            "|    fps             | 78       |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 1046     |\n",
            "|    total_timesteps | 81920    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=82000, episode_reward=-1411.30 +/- 402.27\n",
            "Episode length: 671.20 +/- 96.51\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 671         |\n",
            "|    mean_reward          | -1.41e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 82000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005784146 |\n",
            "|    clip_fraction        | 0.039       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.15       |\n",
            "|    explained_variance   | 0.029       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 140         |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.00556    |\n",
            "|    value_loss           | 298         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=82400, episode_reward=-1415.23 +/- 255.75\n",
            "Episode length: 663.00 +/- 70.03\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 663       |\n",
            "|    mean_reward     | -1.42e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 82400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=82800, episode_reward=-1536.12 +/- 441.92\n",
            "Episode length: 697.80 +/- 101.37\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 698       |\n",
            "|    mean_reward     | -1.54e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 82800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=83200, episode_reward=-1363.57 +/- 369.60\n",
            "Episode length: 638.00 +/- 77.66\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 638       |\n",
            "|    mean_reward     | -1.36e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 83200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=83600, episode_reward=-1699.20 +/- 306.16\n",
            "Episode length: 717.60 +/- 86.28\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 718      |\n",
            "|    mean_reward     | -1.7e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 83600    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=84000, episode_reward=-1546.96 +/- 582.88\n",
            "Episode length: 697.80 +/- 144.37\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 698       |\n",
            "|    mean_reward     | -1.55e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 84000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=84400, episode_reward=-1199.77 +/- 258.18\n",
            "Episode length: 613.00 +/- 48.27\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 613      |\n",
            "|    mean_reward     | -1.2e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 84400    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=84800, episode_reward=-1597.90 +/- 298.59\n",
            "Episode length: 708.00 +/- 65.59\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 708      |\n",
            "|    mean_reward     | -1.6e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 84800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=85200, episode_reward=-1395.97 +/- 227.92\n",
            "Episode length: 656.40 +/- 46.53\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 656      |\n",
            "|    mean_reward     | -1.4e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 85200    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=85600, episode_reward=-1597.24 +/- 192.03\n",
            "Episode length: 702.40 +/- 39.86\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 702      |\n",
            "|    mean_reward     | -1.6e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 85600    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=86000, episode_reward=-1659.54 +/- 189.30\n",
            "Episode length: 703.60 +/- 44.77\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 704       |\n",
            "|    mean_reward     | -1.66e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 86000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=86400, episode_reward=-1819.74 +/- 204.53\n",
            "Episode length: 756.00 +/- 58.47\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 756       |\n",
            "|    mean_reward     | -1.82e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 86400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=86800, episode_reward=-1605.22 +/- 267.23\n",
            "Episode length: 694.00 +/- 52.34\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 694       |\n",
            "|    mean_reward     | -1.61e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 86800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=87200, episode_reward=-1371.93 +/- 379.08\n",
            "Episode length: 661.60 +/- 80.45\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 662       |\n",
            "|    mean_reward     | -1.37e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 87200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=87600, episode_reward=-1643.54 +/- 355.98\n",
            "Episode length: 713.60 +/- 96.84\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 714       |\n",
            "|    mean_reward     | -1.64e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 87600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=88000, episode_reward=-1575.54 +/- 459.66\n",
            "Episode length: 697.60 +/- 120.36\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 698       |\n",
            "|    mean_reward     | -1.58e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 88000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=88400, episode_reward=-1867.15 +/- 182.16\n",
            "Episode length: 762.40 +/- 51.72\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 762       |\n",
            "|    mean_reward     | -1.87e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 88400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=88800, episode_reward=-1423.29 +/- 204.72\n",
            "Episode length: 657.40 +/- 46.34\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 657       |\n",
            "|    mean_reward     | -1.42e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 88800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=89200, episode_reward=-1346.32 +/- 231.94\n",
            "Episode length: 641.00 +/- 53.34\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 641       |\n",
            "|    mean_reward     | -1.35e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 89200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=89600, episode_reward=-1664.15 +/- 296.69\n",
            "Episode length: 720.00 +/- 71.54\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 720       |\n",
            "|    mean_reward     | -1.66e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 89600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=-1582.55 +/- 358.45\n",
            "Episode length: 687.40 +/- 81.87\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 687       |\n",
            "|    mean_reward     | -1.58e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 90000     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 270      |\n",
            "|    ep_rew_mean     | -83.4    |\n",
            "| time/              |          |\n",
            "|    fps             | 75       |\n",
            "|    iterations      | 11       |\n",
            "|    time_elapsed    | 1192     |\n",
            "|    total_timesteps | 90112    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=90400, episode_reward=-1434.98 +/- 171.39\n",
            "Episode length: 957.80 +/- 81.92\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 958          |\n",
            "|    mean_reward          | -1.43e+03    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 90400        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0067716427 |\n",
            "|    clip_fraction        | 0.0311       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.17        |\n",
            "|    explained_variance   | -0.0442      |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 103          |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -0.00368     |\n",
            "|    value_loss           | 246          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=90800, episode_reward=-1181.32 +/- 389.44\n",
            "Episode length: 819.80 +/- 173.98\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 820       |\n",
            "|    mean_reward     | -1.18e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 90800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=91200, episode_reward=-1316.71 +/- 190.79\n",
            "Episode length: 872.60 +/- 75.16\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 873       |\n",
            "|    mean_reward     | -1.32e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 91200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=91600, episode_reward=-1319.78 +/- 239.49\n",
            "Episode length: 867.40 +/- 96.56\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 867       |\n",
            "|    mean_reward     | -1.32e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 91600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=92000, episode_reward=-1328.83 +/- 112.57\n",
            "Episode length: 880.20 +/- 41.82\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 880       |\n",
            "|    mean_reward     | -1.33e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 92000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=92400, episode_reward=-1164.50 +/- 461.61\n",
            "Episode length: 805.60 +/- 224.53\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 806       |\n",
            "|    mean_reward     | -1.16e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 92400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=92800, episode_reward=-1151.36 +/- 230.18\n",
            "Episode length: 806.40 +/- 93.46\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 806       |\n",
            "|    mean_reward     | -1.15e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 92800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=93200, episode_reward=-1178.41 +/- 422.32\n",
            "Episode length: 825.00 +/- 209.79\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 825       |\n",
            "|    mean_reward     | -1.18e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 93200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=93600, episode_reward=-1126.08 +/- 404.27\n",
            "Episode length: 786.40 +/- 182.53\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 786       |\n",
            "|    mean_reward     | -1.13e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 93600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=94000, episode_reward=-1206.70 +/- 195.79\n",
            "Episode length: 849.80 +/- 82.41\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 850       |\n",
            "|    mean_reward     | -1.21e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 94000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=94400, episode_reward=-1092.52 +/- 262.39\n",
            "Episode length: 799.40 +/- 120.65\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 799       |\n",
            "|    mean_reward     | -1.09e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 94400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=94800, episode_reward=-1221.81 +/- 296.80\n",
            "Episode length: 843.40 +/- 135.97\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 843       |\n",
            "|    mean_reward     | -1.22e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 94800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=95200, episode_reward=-1379.72 +/- 133.57\n",
            "Episode length: 924.20 +/- 66.10\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 924       |\n",
            "|    mean_reward     | -1.38e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 95200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=95600, episode_reward=-976.29 +/- 442.60\n",
            "Episode length: 729.20 +/- 196.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 729      |\n",
            "|    mean_reward     | -976     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95600    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96000, episode_reward=-1249.66 +/- 314.23\n",
            "Episode length: 856.20 +/- 119.24\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 856       |\n",
            "|    mean_reward     | -1.25e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 96000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=96400, episode_reward=-1159.03 +/- 301.03\n",
            "Episode length: 815.60 +/- 116.14\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 816       |\n",
            "|    mean_reward     | -1.16e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 96400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=96800, episode_reward=-1066.44 +/- 287.07\n",
            "Episode length: 767.00 +/- 123.81\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 767       |\n",
            "|    mean_reward     | -1.07e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 96800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=97200, episode_reward=-1194.84 +/- 301.13\n",
            "Episode length: 830.80 +/- 119.37\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 831       |\n",
            "|    mean_reward     | -1.19e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 97200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=97600, episode_reward=-1121.09 +/- 394.99\n",
            "Episode length: 779.20 +/- 178.45\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 779       |\n",
            "|    mean_reward     | -1.12e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 97600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=98000, episode_reward=-1255.23 +/- 378.77\n",
            "Episode length: 833.00 +/- 158.49\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 833       |\n",
            "|    mean_reward     | -1.26e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 98000     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 305      |\n",
            "|    ep_rew_mean     | -67.3    |\n",
            "| time/              |          |\n",
            "|    fps             | 71       |\n",
            "|    iterations      | 12       |\n",
            "|    time_elapsed    | 1384     |\n",
            "|    total_timesteps | 98304    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98400, episode_reward=-802.30 +/- 265.29\n",
            "Episode length: 611.20 +/- 156.09\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 611       |\n",
            "|    mean_reward          | -802      |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 98400     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0089767 |\n",
            "|    clip_fraction        | 0.0688    |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -1.09     |\n",
            "|    explained_variance   | 0.0354    |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 116       |\n",
            "|    n_updates            | 120       |\n",
            "|    policy_gradient_loss | -0.00618  |\n",
            "|    value_loss           | 199       |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=98800, episode_reward=-723.64 +/- 340.95\n",
            "Episode length: 548.80 +/- 192.30\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 549      |\n",
            "|    mean_reward     | -724     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99200, episode_reward=-951.35 +/- 71.09\n",
            "Episode length: 693.40 +/- 53.47\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 693      |\n",
            "|    mean_reward     | -951     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99200    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99600, episode_reward=-794.24 +/- 332.50\n",
            "Episode length: 584.60 +/- 191.13\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 585      |\n",
            "|    mean_reward     | -794     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99600    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=-1005.75 +/- 216.38\n",
            "Episode length: 707.20 +/- 112.75\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 707       |\n",
            "|    mean_reward     | -1.01e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 100000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=100400, episode_reward=-858.82 +/- 150.78\n",
            "Episode length: 620.60 +/- 81.09\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 621      |\n",
            "|    mean_reward     | -859     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 100400   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=100800, episode_reward=-1154.44 +/- 172.24\n",
            "Episode length: 773.00 +/- 84.28\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 773       |\n",
            "|    mean_reward     | -1.15e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 100800    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=101200, episode_reward=-806.12 +/- 198.35\n",
            "Episode length: 590.40 +/- 114.10\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 590      |\n",
            "|    mean_reward     | -806     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 101200   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=101600, episode_reward=-687.15 +/- 332.60\n",
            "Episode length: 529.40 +/- 194.90\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 529      |\n",
            "|    mean_reward     | -687     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 101600   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=102000, episode_reward=-791.80 +/- 227.08\n",
            "Episode length: 603.40 +/- 129.41\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 603      |\n",
            "|    mean_reward     | -792     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 102000   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=102400, episode_reward=-785.66 +/- 197.58\n",
            "Episode length: 592.60 +/- 116.08\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 593      |\n",
            "|    mean_reward     | -786     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 102400   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=102800, episode_reward=-789.65 +/- 257.48\n",
            "Episode length: 588.20 +/- 136.91\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 588      |\n",
            "|    mean_reward     | -790     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 102800   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=103200, episode_reward=-989.49 +/- 309.26\n",
            "Episode length: 701.20 +/- 174.61\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 701      |\n",
            "|    mean_reward     | -989     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 103200   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=103600, episode_reward=-676.80 +/- 242.73\n",
            "Episode length: 521.60 +/- 128.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 522      |\n",
            "|    mean_reward     | -677     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 103600   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=104000, episode_reward=-905.97 +/- 244.70\n",
            "Episode length: 663.80 +/- 141.11\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 664      |\n",
            "|    mean_reward     | -906     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 104000   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=104400, episode_reward=-1036.11 +/- 204.68\n",
            "Episode length: 709.00 +/- 117.69\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 709       |\n",
            "|    mean_reward     | -1.04e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 104400    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=104800, episode_reward=-751.47 +/- 383.25\n",
            "Episode length: 582.20 +/- 209.96\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 582      |\n",
            "|    mean_reward     | -751     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 104800   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=105200, episode_reward=-904.41 +/- 402.71\n",
            "Episode length: 638.20 +/- 230.34\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 638      |\n",
            "|    mean_reward     | -904     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 105200   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=105600, episode_reward=-943.37 +/- 184.60\n",
            "Episode length: 669.60 +/- 102.41\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 670      |\n",
            "|    mean_reward     | -943     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 105600   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=106000, episode_reward=-797.61 +/- 204.31\n",
            "Episode length: 581.60 +/- 115.30\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 582      |\n",
            "|    mean_reward     | -798     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 106000   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=106400, episode_reward=-992.00 +/- 281.04\n",
            "Episode length: 707.40 +/- 149.22\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 707      |\n",
            "|    mean_reward     | -992     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 106400   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 324      |\n",
            "|    ep_rew_mean     | -59.1    |\n",
            "| time/              |          |\n",
            "|    fps             | 70       |\n",
            "|    iterations      | 13       |\n",
            "|    time_elapsed    | 1512     |\n",
            "|    total_timesteps | 106496   |\n",
            "---------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "Cz9MxM8-Vc0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "x8GnkAOtOc9q"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logs_dqn = np.load('evaluations_dqn.npz', allow_pickle=True)\n",
        "logs_ppo = np.load('evaluations_ppo.npz', allow_pickle=True)"
      ],
      "metadata": {
        "id": "aDoG76tXOrIc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reward_dqn = np.array(logs_dqn['results'])\n",
        "reward_ppo = np.array(logs_ppo['results'])"
      ],
      "metadata": {
        "id": "pJd7NBuNOzdi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reward_dqn_flat = reward_dqn.flatten()\n",
        "timesteps_dqn = np.arange(len(reward_dqn_flat))\n",
        "df_dqn = pd.DataFrame({'timesteps': timesteps_dqn, 'rewards': reward_dqn_flat})"
      ],
      "metadata": {
        "id": "s4prgCnaO-7J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reward_ppo_flat = reward_ppo.flatten()\n",
        "timesteps_ppo = np.arange(len(reward_ppo_flat))\n",
        "df_ppo = pd.DataFrame({'timesteps': timesteps_ppo, 'rewards': reward_ppo_flat})"
      ],
      "metadata": {
        "id": "Vb2kD3n_PXEA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create Matplotlib plot for DQN rewards\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(df_dqn['timesteps'], df_dqn['rewards'], label='DQN')\n",
        "plt.title('Rewards over Timesteps (DQN)')\n",
        "plt.xlabel('Timesteps')\n",
        "plt.ylabel('Rewards')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Create Matplotlib plot for PPO rewards\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(df_ppo['timesteps'], df_ppo['rewards'], label='PPO')\n",
        "plt.title('Rewards over Timesteps (PPO)')\n",
        "plt.xlabel('Timesteps')\n",
        "plt.ylabel('Rewards')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 943
        },
        "id": "AwfQZNz4UvO0",
        "outputId": "33f10267-12f8-47d2-b739-c80e4cc99faa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}